---
title: "Machine Learning Computer Lab 2 (Group A7)"
author: 
  - Qinyuan Qi(qinqi464)
  - Satya Sai Naga Jaya Koushik	Pilla (satpi345)
  - Daniele	Bozzoli(danbo826)  
date: "`r Sys.Date()`"
output: pdf_document
---

## Statement of Contribution
For Lab2, we decided to split the three assignments equally, Qinyuan completed Assignment 1, Satya
completed Assignment 2 and Daniele completed Assignment 3, after which, for verification sake, we completed
each otherâ€™s assignments as well and validated our findings. The report was also compiled by three of us,
with each handling their respective assignments.


## Assignment 1: Explicit regularization
### Answer:

```{r setup1, echo=FALSE, cache=TRUE}
###########################  Init code For  Assignment 1 #######################
rm(list = ls())
library(plyr)
library(readr)
library(dplyr)
library(caret)
library(ggplot2)
library(repr)
library(glmnet)
library(rpart)
library(rpart.plot)

# set random seed
set.seed(12345)
```

### (1)

According to the question, we can model create a linear model like the following.

$$
Fat = \beta_{0} + \beta_{1} Channel_{1} + \beta_{2}Channel_{2}+ ... + \beta_{100}Channel_{100} + \epsilon = \sum{\beta_{i} Channel_{i}} + \epsilon
$$

In the above formula, $\beta_{0}$ is the intercept, while the remaing $\beta$ are the cofeeicients corresponding to each channel.

According to the output, the model generated use 100 channel features, and almost all the channels provide contributions to the target, however,  p value shows that only very limited channels are useful. And the  $MSE_{test} =  722.4294$ , $MSE_{training} = 0.005709117$. It means training data fit pretty well,however the test data fit not as expected, and the model overfit the data.

```{r 1.1, echo=FALSE,cache=TRUE}
######################  Assignment 1.1 #########################################


# read data
data <- read.csv("tecator.csv")
data <- data %>% select(Fat,Channel1:Channel100)

row_num <- nrow(data)
# set data split ratio to 0.5, 0.5
ratio <- c(train = .5, test = .5)

# split data to training and test dataset
train_id <- sample(1:row_num, floor(row_num * ratio[1]))
train_data_set <- data[train_id, ]

test_id <- setdiff(1:row_num, train_id)
test_data_set <- data[test_id, ]

# fit linear model
lm_model <- lm(Fat ~ ., data = train_data_set)

predicted_fat_test <- predict(lm_model, test_data_set)
predicted_fat_train <- predict(lm_model, train_data_set)

# calc the mean value
test_mse <- mean((predicted_fat_test - test_data_set$Fat)^2) 
train_mse <- mean((predicted_fat_train - train_data_set$Fat)^2) 
cat("test_mse is:",test_mse,"\n")
cat("train_mse is:",train_mse,"\n")

summary(lm_model)
```

### (2)
The cost function in lasso regression is:

$$
\hat{\theta} = argmin_{\theta} \frac{1}{n} || X\theta||^{2} + \lambda ||\theta||_{1}
$$

### (3)

According to the plot of lasso_reg, we find that when $log(\lambda)$ is small, degree of freedom is big which
means more features explain the target variable, but when $log(\lambda)$ increase, the degree of freedom decrease 
at the same time, and coefficient get close to 0 in most of the time. But we also find that some of the features's 
coefficient line fluctuate when it is less than 0. But all of the features's coefficient will converge to 0 at last.

```{r 1.3.1, echo=FALSE,cache=TRUE}
######################  Assignment 1.3 #########################################
# regularize the data before we fit the model
pre_proc_val <- preProcess(train_data_set, method = c("center", "scale"))
train_data_set <- predict(pre_proc_val, train_data_set)
test_data_set <- predict(pre_proc_val, test_data_set)

x_train <- as.matrix(train_data_set %>% select(-Fat))
y_train <- as.matrix(train_data_set %>% select(Fat))

# when alpha = 1, it is lasso regression
lasso_reg <- glmnet(x_train, y_train, alpha = 1,family = "gaussian")

#summary(lasso_reg)

```

According to the calculation and output of the code, we find that lambda is: 0.05123599 $log(\lambda)$ is -2.971313 when feature number is 4 (including the intercept.

```{r 1.3.2,echo=FALSE,cache=TRUE}

lambda_deg_freedom <- data.frame(lambda = lasso_reg$lambda,
                                 deg_freedom = lasso_reg$df)

lambda_deg_freedom <- lambda_deg_freedom %>% arrange(desc(deg_freedom))

lambda_deg_freedom_num <- lambda_deg_freedom[which(lambda_deg_freedom$deg_freedom == 4),] %>% 
                        head(1) %>% pull(lambda)

cat("lambda is ",lambda_deg_freedom_num,
    " log(lambda) is ",log(lambda_deg_freedom_num),
    "\nwhen feature number is 4 (including the intercept)\n")

plot(lasso_reg, xvar="lambda", xlab='Log Lambda')
abline(v=log(lambda_deg_freedom_num), col="red")
```

### (4)

the cost function in ridge regression is:

$$
\hat{\theta} = argmin_{\theta} \frac{1}{n} || X\theta||^{2} + \lambda ||\theta||_{2}^{2}
$$

For ridge regression, we find that the degree of freedom always keep same when $log(\lambda)$ increase,
but the coefficient of features seems to will converge to 0, but after check beta value of ridge_reg, we found 
that the coefficient of features will not converge to 0, but will become a very small value.

Compare to lasso regression, we can not find the required lambda value when degree of freedom is 4, since coefficient
value not converge to 0, but converge to a very small value.

```{r 1.4,echo=FALSE,cache=TRUE}
######################  Assignment 1.4 #########################################
# when alpha = 0, it is ridge regression
ridge_reg <- glmnet(x_train, y_train, alpha = 0,family = "gaussian")

lambda_deg_freedom <- data.frame(lambda = ridge_reg$lambda,
                                 deg_freedom = ridge_reg$df)

lambda_deg_freedom <- lambda_deg_freedom %>% arrange(desc(deg_freedom))

lambda_deg_freedom_num <- lambda_deg_freedom[which(lambda_deg_freedom$deg_freedom == 4),] %>% 
                        head(1) %>% pull(lambda)

plot(ridge_reg,xvar="lambda", xlab='Log Lambda')
```

### (5)
The plot as follows, when log(lambda) increase, CV score increase.
Optimized lambda is -5.39019, and in this case, around 10 variables were chosen.

When lambda = -4, MSE is higher than when lambda=-5.39019. we can say that it results 
in a statistically significantly better prediction.

The scatter plot as follow.

Since almost all points are around the 45-degree line,it indicate accurate predictions.

```{r 1.5,warning=FALSE, cache=TRUE}
# when alpha = 1, it is lasso regression
cv_lasso_model <- cv.glmnet(x, y_train, alpha = 1) 
summary(cv_lasso_model)

# Extract lambda values and CV scores
lambda_values <- log(cv_lasso_model$lambda)
cv_scores <- cv_lasso_model$cvm

# Plot the dependence of CV score on log(lambda)
plot(cv_lasso_model,xlab = "log lambda")
plot(lambda_values, cv_scores, type = "b", xlab = "log lambda ", ylab = "CV Score",main = "CV Score vs log(lambda)")

min_cv_lambda <- log(cv_lasso_model$lambda.min)
min_cv_score <- min(cv_scores)
cat("min_cv_lambda is:",min_cv_lambda,"\n")
points(min_cv_lambda, min_cv_score, col = "red", pch = 16, cex = 1.5)
text(min_cv_lambda, min_cv_score, "Minimum CV Score", pos = 3, col = "red")

predictions <- predict(cv_lasso_model, newx = as.matrix(x_test), s = "lambda.min")

plot(y_test, predictions, pch = 16, col = "red",
     xlab = "Original Values", ylab = "Predicted  Values",
     main = "Scatter Plot of Test Values for LASSO Model")
abline(a = 0, b = 1, col = "blue", lty = 2)
```

## Assignment 2: Decision trees and logistic regression for bank marketing

### Answer:

```{r setup2, include=FALSE}
###########################  Init code For  Assignment 1 #######################
rm(list = ls())

```

### (1)
```{r 2.1, cache=TRUE}
######################  Assignment 2.1 #########################################
# read data
data <- read.csv("bank-full.csv",header = TRUE, sep = ";")

row_num <- nrow(data)
cols_num <- ncol(data)

# set data split ratio to 0.4, 0.3, 0.3
ratio <- c(train = .4, validate = 0.3, test = .3)

# set random seed
set.seed(12345)

# split data to training,validate and test dataset
train_id <- sample(1:row_num, floor(row_num * ratio[1]))
train_set <- data[train_id, ]

# set random seed
set.seed(12345)

validation_test_id <- setdiff(1:row_num, train_id)
validation_id <- sample(validation_test_id, floor(row_num * ratio[2]))
validation_set <- data[validation_id, ]

test_id <- setdiff(validation_test_id, validation_id)
test_set <- data[test_id, ]

```

### (2)

Decision trees are as follows.

misclassification is listed below.

||misclassification rate of train|misclassification rate of validation|
|:-:|:-:|:-:|
|a|0.0995355|0.0995355|
|b|0.1142446|0.1207697|
|c|0.06287326|0.1041805|

According to the data in the table above, the first one has a good misclassification rate of train and validation.
the last one has a good misclassification rate of train but a  little bit large  misclassification rate on validation set. b will not be considered, since both of values are biggest among 3 methods.

```{r 2.2, cache=TRUE}
######################  Assignment 2.2 #########################################
tree_a <- rpart(y ~ ., data = train_set, method = "class")
tree_b <- rpart(y ~ ., data = train_set, method = "class", control = rpart.control(minbucket = 7000))
tree_c <- rpart(y ~ ., data = train_set, method = "class", control = rpart.control(cp = 0.0005))

train_predictions_a <- predict(tree_a, train_set, type = "class")
validation_predictions_a <- predict(tree_a, validation_set, type = "class")

train_predictions_b <- predict(tree_b, train_set, type = "class")
validation_predictions_b <- predict(tree_b, validation_set, type = "class")

train_predictions_c <- predict(tree_c, train_set, type = "class")
validation_predictions_c <- predict(tree_c, validation_set, type = "class")

# misclassification rates
train_misclassification_rate_a <- mean(train_predictions_a != train_set$y)
validation_misclassification_rate_a <- mean(validation_predictions_a != validation_set$y)

train_misclassification_rate_b <- mean(train_predictions_b != train_set$y)
validation_misclassification_rate_b <- mean(validation_predictions_b != validation_set$y)

train_misclassification_rate_c <- mean(train_predictions_c != train_set$y)
validation_misclassification_rate_c <- mean(validation_predictions_c != validation_set$y)



cat("train_misclassification_rate_a is ",train_misclassification_rate_a,"\n")
cat("validation_misclassification_rate_a is ",validation_misclassification_rate_a,"\n")

cat("train_misclassification_rate_b is ",train_misclassification_rate_b,"\n")
cat("validation_misclassification_rate_b is ",validation_misclassification_rate_b,"\n")

cat("train_misclassification_rate_c is ",train_misclassification_rate_c,"\n")
cat("validation_misclassification_rate_c is ",validation_misclassification_rate_c,"\n")

rpart.plot(tree_a)
rpart.plot(tree_b)
rpart.plot(tree_c)
```

### (3)

```{r 2.3, cache=TRUE}
######################  Assignment 2.3 #########################################
# Set folds number
num_folds <- 5
cv_inds <- cut(seq(1, nrow(train_set)), breaks = num_folds, labels = FALSE)
cv_errors <- numeric(30)
train_deviances_depth <- numeric(30)
validation_deviances_depth <- numeric(30)

for (depth in 1:30) {  
  tree <- rpart(y ~ ., data = train_set, method = "class", 
  control = rpart.control(cp = 0.0005), maxdepth = depth)
        
  validation_predictions <- predict(tree, validation_set, type = "class")
    
  cv_errors_depth <- mean(validation_predictions != validation_set$y)
    
  cv_errors[depth] <- cv_errors_depth

  train_deviances_depth[depth] <- sum(deviance(tree))
  validation_deviances_depth[depth] <- sum(deviance(tree, newdata = validation_set))
}

optimal_depth <- which.min(cv_errors)
cat("Optimal tree depth:", optimal_depth, "\n")

plot_data <- data.frame(Leaves = 1:30, Train_Deviance = train_deviances_depth, 
Validation_Deviance = validation_deviances_depth)

ggplot(plot_data, aes(x = Leaves)) +
  geom_line(aes(y = Train_Deviance, color = "Training Data"), size = 1) +
  geom_line(aes(y = Validation_Deviance, color = "Validation Data"), size = 1) +
  labs(title = "Dependence of Deviances vs the Number of Leaves",
       x = "Number of Leaves",
       y = "Deviance") +
  scale_color_manual(values = c("Training Data" = "blue", "Validation Data" = "red"))
```

### (4)
An optimized tree depth = 1.
```{r 2.4.1, cache=TRUE}
######################  Assignment 2.4 #########################################
final_tree <- rpart(y ~ ., data = train_set, method = "class", 
control = rpart.control(cp = 0.0005), maxdepth = 1)

```

```{r 2.4.2,eval=FALSE, cache=TRUE}
validation_predictions <- predict(final_tree, validation_set, type = "class")

conf_matrix <- confusionMatrix(validation_predictions, validation_set$y)

accuracy <- conf_matrix$overall["Accuracy"]
f1_score <- conf_matrix$byClass["F1"]

# Print the confusion matrix, accuracy, and F1 score
cat("Confusion Matrix:\n")
cat(conf_matrix$table,"\n")

cat("Accuracy:", accuracy, "\n")
cat("F1 Score:", f1_score, "\n")
```

### (5)

```{r 2.5, cache=TRUE}
######################  Assignment 2.5 #########################################
loss_matrix <- matrix(c(0, 1, 5, 0), nrow = 2)
test_predictions <- predict(final_tree, test_set, type = "class", parms = list(loss = loss_matrix))
conf_matrix <- table(Actual = test_set$y, Predicted = test_predictions)
cat("Confusion Matrix:\n")
print(conf_matrix)

```

### (6)
```{r 2.6,eval=FALSE, cache=TRUE}
######################  Assignment 2.6 #########################################
threshold <- 0.05  
logistic_model <- glm(y ~ ., data = train_set, family = "binomial")

logistic_probabilities <- predict(logistic_model, test_set, type = "response")
logistic_predictions <- ifelse(logistic_probabilities > threshold, "yes", "no")

conf_matrix_logistic <- confusionMatrix(logistic_predictions, test_set$y)

TP <- conf_matrix_logistic$table["yes", "yes"]
FN <- conf_matrix_logistic$table["no", "yes"]
FP <- conf_matrix_logistic$table["yes", "no"]
TN <- conf_matrix_logistic$table["no", "no"]

TPR <- TP / (TP + FN)
FPR <- FP / (FP + TN)

# Print the results
cat("True Positive Rate (TPR):", TPR, "\n")
cat("False Positive Rate (FPR):", FPR, "\n")
```

## Assignment 3. Principal components and implicit regularization

### Answer:

```{r setup3, include=FALSE, cache=TRUE}
###########################  Init code For  Assignment 3 #######################
rm(list=ls(all.names = T))
library(ggplot2)
library(caret)
set.seed(12345)
data <- read.csv("communities.csv")
```

### (1)
According to the output, we need 35 components to obtain at least 95% of variance in the data. The proportion of variation explained by first and second principal components are 0.2501699 and 0.1693597 respectively.

```{r 3.1, cache=TRUE}
######################  Assignment 3.1 #########################################
n <- nrow(data)
features <- data[, -101]

s_features <- scale(features)

S <- (t(s_features) %*% s_features)/n  # sample covariance matrix

Eig <- eigen(S) 

# eigen in descending order
s_indx <- order(Eig$values, decreasing = TRUE)
s_eig <- Eig$values[s_indx]

# cumulative explained variance
cum_var <- cumsum(s_eig) / sum(s_eig)

q_95 <- which(cum_var >= 0.95)[1]  # q for 95% var

first_two_components <- Eig$vectors[, 1:2]  # first two PC

# proportion of variation explained by each of the first two components
PC1_var <- s_eig[1] / sum(s_eig)
PC2_var <- s_eig[2] / sum(s_eig)

cat("Number of components needed for 95% variance:", q_95, "
Proportion of variation explained by the first component:", PC1_var, "
Proportion of variation explained by the second component:", PC2_var, "\n")
```

### (2)

We observe from the plot how the variable "state", the first feature of PC1, 
explains most of the data. The other variables carry significantly less 
explanation. The other 4 features that contribute the most are features 93 
(PctBornSameState), 61 (PctSpeakEnglOnly), 5 (racePctWhite) and 76 
(PersPerRentOccHous).

```{r 3.2, cache=TRUE}
######################  Assignment 3.2 #########################################
PCA <- princomp(features)
L <- PCA$loadings
plot(L[,1], xlim=c(0,20))
highest5 <- order(L[,1], decreasing=TRUE)[1:5]
plot(L[,1], L[,2], main="PCA Scores")
```

### (3)

Train MSE is 0.2752071 and Test MES is 0.4248011.
We observe how the MSE of the test data is significantly bigger than
the MSE obtained from train data. This might happen because the model chosen is overfitting our given data.

```{r 3.3, cache=TRUE}
######################  Assignment 3.3 #########################################
# train and test data
id <- sample(1:n, floor(n*0.5))
trn <- data[id,]
tst <- data[-id,]

# scaling
scaler <- preProcess(trn)
trainS <- predict(scaler,trn)
testS <- predict(scaler,tst)

# linear regression model and test data predictions
linmod <- lm(trainS$ViolentCrimesPerPop ~ ., trainS)
test_pred <- predict(linmod, testS[,-101])

# training and test data MSE
train_MSE <- mean((trainS$ViolentCrimesPerPop - linmod$fitted.values)^2)
test_MSE <- mean((testS$ViolentCrimesPerPop - test_pred)^2)

cat("Train mean squared error:", train_MSE, "\nTest mean squared error:", test_MSE)

```

### (4)

Looking at the plots we can see how the errors converge to zero
after 1700 iterations (approx 1200 in the second graph but considering that we took off
the first 500). Hence 1700 is the optimal iteration number to get good results.
The following iterations do not significally improve our model
and hence can led us to overfitting.


```{r 3.4, cache=TRUE}
######################  Assignment 3.4 #########################################
train_new <- as.matrix(trainS[,-101])  # training data - response variable
train_r <- trainS[,101] # training response variable

test_new <- as.matrix(testS[,-101])  # test data - response variable
test_r <- trainS[,101] # test response variable

# error vectors for training and test data
train_e <- c()
test_e <- c()

costfun <- function(theta_vec){
  train_cost <- mean(((train_new %*% theta_vec) - train_r)^2)
  train_e <<- c(train_e, train_cost)
  test_cost <- mean(((test_new %*% theta_vec) - test_r)^2)
  test_e <<- c(test_e, test_cost)
  return(train_cost)
}

theta0 <- rep(0,100)
opt <- optim(theta0, method="BFGS", costfun)

opt$val

par(mfrow=c(1,2))

plot(ylab="errors", xlab="iteration",train_e, type = "l", main="Errors")
lines(test_e, col="red")
legend("topright", legend = c("Train errors", "Test errors"), 
col = c("black", "red"), lty = 1, cex = 0.5)

plot(ylab="errors", xlab="iteration",train_e[-c(1:500)][1:2000], 
type = "l", main="Zoom on errors")
lines(test_e[-c(1:500)][1:2000], col="red")
legend("topright", legend = c("Train errors", "Test errors"), 
col = c("black", "red"), lty = 1, cex = 0.5)

```



\newpage
# Appendix: All code for this report

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```
