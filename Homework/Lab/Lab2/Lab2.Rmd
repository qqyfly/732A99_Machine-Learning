---
title: "Machine Learning Computer Lab 2 (Group A7)"
author: 
  - Qinyuan Qi(qinqi464)
  - Satya Sai Naga Jaya Koushik	Pilla (satpi345)
  - Daniele	Bozzoli(danbo826)  
date: "`r Sys.Date()`"
output: pdf_document
---

## Statement of Contribution
For Lab 2, we decided to split the three assignments equally, Qinyuan completed Assignment 1, Satya
completed Assignment 2 and Daniele completed Assignment 3, after which, for verification's sake, we completed
each other's assignments as well and validated our findings. The report was also compiled by three of us,
with each handling their respective assignments.


## Assignment 1: Explicit regularization
### Answer:

```{r setup1, echo=FALSE, cache=TRUE}
###########################  Init code For  Assignment 1 #######################
rm(list = ls())
library(plyr)
library(readr)
library(dplyr)
library(caret)
library(ggplot2)
library(repr)
library(glmnet)
library(rpart)
library(tree)

# set random seed
set.seed(12345)
```

### (1)

According to the question, we can create a linear model like the following.

$$
Fat = \beta_{0} + \beta_{1} Channel_{1} + \beta_{2}Channel_{2}+ ... + \beta_{100}Channel_{100} + \epsilon = \sum{\beta_{i} Channel_{i}} + \epsilon
$$

In the above formula, $\beta_{0}$ is the intercept, while the remaining $\beta$ are the parameters corresponding to each channel.

According to the output, the model generated uses 100 channel features, and almost all the channels provide contributions to the target.
However, the p-value shows that only very limited channels are useful. And the  $MSE_{test} =  722.4294$ , $MSE_{training} = 0.005709117$. 
It means the training data fit pretty well, however, the test data fit not as expected, and the model overfits the data.

```{r 1.1, echo=FALSE, cache=TRUE}
######################  Assignment 1.1 #########################################


# read data
data <- read.csv("tecator.csv")
data <- data %>% select(Fat,Channel1:Channel100)

row_num <- nrow(data)
# set data split ratio to 0.5, 0.5
ratio <- c(train = .5, test = .5)

# split data to the train dataset and the test dataset
train_id <- sample(1:row_num, floor(row_num * ratio[1]))
train_data_set <- data[train_id, ]

test_id <- setdiff(1:row_num, train_id)
test_data_set <- data[test_id, ]

# fit linear model
lm_model <- lm(Fat ~ ., data = train_data_set)

predicted_fat_test <- predict(lm_model, test_data_set)
predicted_fat_train <- predict(lm_model, train_data_set)

# calc the mean value
test_mse <- mean((predicted_fat_test - test_data_set$Fat)^2) 
train_mse <- mean((predicted_fat_train - train_data_set$Fat)^2) 
cat("test_mse is:",test_mse,"\n")
cat("train_mse is:",train_mse,"\n")

summary(lm_model)
```

### (2)
The cost function in lasso regression is:

$$
\hat{\theta} = argmin_{\theta} \frac{1}{n} || X\theta||^{2} + \lambda ||\theta||_{1}
$$

### (3)

According to the plot of lasso_reg, we find that when $log(\lambda)$ is small, the degree of freedom is big which
means more features explain the target variable, but when $log(\lambda)$ increases, the degree of freedom decrease 
at the same time, and coefficient get close to 0 in most of the time. We also find that some of the feature's 
coefficient line fluctuates when it is less than 0. But all of the features's coefficients will converge to 0 at last.

```{r 1.3.1, echo=FALSE, cache=TRUE}
######################  Assignment 1.3 #########################################
x_train <- as.matrix(train_data_set %>% select(-Fat))
y_train <- as.matrix(train_data_set %>% select(Fat))

# when alpha = 1, it is lasso regression
lasso_reg <- glmnet(x_train, y_train, alpha = 1,family = "gaussian")
```

```{r 1.3.2,echo=FALSE, cache=TRUE}

lambda_deg_freedom <- data.frame(lambda = lasso_reg$lambda,
                                 deg_freedom = lasso_reg$df)

lambda_deg_freedom <- lambda_deg_freedom %>% arrange(desc(deg_freedom))

lambda_deg_freedom_num <- lambda_deg_freedom[which(lambda_deg_freedom$deg_freedom == 4),] %>% 
                        head(1) %>% pull(lambda)

#cat("lambda is ",lambda_deg_freedom_num,
#    " log(lambda) is ",log(lambda_deg_freedom_num),
#    "\nwhen feature number is 4 (including the intercept)\n")
```

According to the calculation and output of the code, we find that when the feature number is 4 (including the intercept),
lambda is: `r lambda_deg_freedom_num` and $log(\lambda)$ is `r log(lambda_deg_freedom_num)` .

```{r 1.3.3,echo=FALSE, cache=TRUE}
plot(lasso_reg, xvar="lambda", xlab='Log Lambda')
abline(v=log(lambda_deg_freedom_num), col="red")
```

### (4)

The cost function in ridge regression is:

$$
\hat{\theta} = argmin_{\theta} \frac{1}{n} || X\theta||^{2} + \lambda ||\theta||_{2}^{2}
$$

For ridge regression, we find that the degree of freedom always keeps the same when $log(\lambda)$ increases,
but the coefficient of features seems to converge to 0, after checking the beta value of ridge_reg, we found 
that the coefficient of features will not converge to 0, it will become a very small value.

Compared to lasso regression, we can not find the required lambda value when the degree of freedom is 4, since the coefficient
value not converge to 0, but converges to a very small value as stated above.

```{r 1.4,echo=FALSE, cache=TRUE}
######################  Assignment 1.4 #########################################
# when alpha = 0, it is ridge regression
ridge_reg <- glmnet(x_train, y_train, alpha = 0,family = "gaussian")

lambda_deg_freedom <- data.frame(lambda = ridge_reg$lambda,
                                 deg_freedom = ridge_reg$df)

lambda_deg_freedom <- lambda_deg_freedom %>% arrange(desc(deg_freedom))

lambda_deg_freedom_num <- lambda_deg_freedom[which(lambda_deg_freedom$deg_freedom == 4),] %>% 
                        head(1) %>% pull(lambda)

plot(ridge_reg,xvar="lambda", xlab='Log Lambda')
```

### (5)
The plot is as follows, when $log(\lambda)$ increases, features explaining the model get fewer, and the model becomes simpler.

From the plot, we also find that when there are at least 9 features, the MSE is very stable and keeps at a lower level. 
However, when $log(\lambda)$ increases, MSE increases rapidly until 3 features. It means those 9 features are significant.

```{r 1.5.1,echo=FALSE,out.height="250px", cache=TRUE,fig.align = 'center'}
# when alpha = 1, it is lasso regression
cv_lasso_model <- cv.glmnet(x_train, y_train, alpha = 1, family = "gaussian") 

# Plot the dependence of CV score on log(lambda)
plot(cv_lasso_model,xlab = "log lambda")
```

```{r 1.5.2,echo=FALSE, cache=TRUE}
# Extract lambda values and CV scores
opt_lambda <- cv_lasso_model$lambda.min
opt_features <- cv_lasso_model$nzero[which(cv_lasso_model$lambda == opt_lambda)]

#cat("Optimal log(lambda) is:",log(opt_lambda),"\n")
#cat("Corresponding features used is:", opt_features,"\n")
```

And optimized $\lambda$ is  `r opt_lambda` , $log(\lambda)$ is `r log(opt_lambda)`, in this case, `r opt_features` features were chosen.

Compare the optimized $\lambda$ and corresponding $log(\lambda)$ with the $log(\lambda) = -4$, according to the plot, it's relatively flat in this range.
So we can say that it does not indicate it's possible for $log(\lambda) = -4$ to get statistical significally better prediction.

To compare the models, a scatter plot was used with different colors. 

As we can see from the plot, the green dots(opt lasso) are closer to the original blue dots than the red dots(linear regression).

This means the linear regression model is not as good as the opt lasso model in this case. 

When Fat(<10) is small, linear regression fits well, when Fat gets bigger, linear regression fit points have a big gap with the original data. 
However, the opt lasso model fits well in all the ranges. So regularization used in lasso helps to improve the model.

```{r 1.5.3,echo=FALSE,fig.align = 'center'}
x_test <- as.matrix(test_data_set %>% select(-Fat))
y_test <- as.matrix(test_data_set %>% select(Fat))

opt_lasso_model <- glmnet(x_train, y_train, alpha = 1, family = "gaussian", lambda = opt_lambda)
opt_lasso_prediction <- predict(opt_lasso_model, 'response', newx = x_test)

test_len <- length(y_test)
scatter_df = data.frame(
              x = rep(y_test, 3),
              y = c(y_test, predicted_fat_test, opt_lasso_prediction),
              Model = rep(c("Original","Linear Regression","Opt Lasso"), each = test_len)
            )
ggplot(scatter_df, aes(x = x, y = y, color = Model)) +
  geom_point() +
  labs(title = "Scatter Plot with Three Sets of Data", x = "Fat", y = "Predicted Value")  + 
  scale_color_discrete(name="") 
```

## Assignment 2: Decision trees and logistic regression for bank marketing

### Answer:

```{r setup2, include=FALSE}
###########################  Init code For  Assignment 1 #######################
rm(list = ls())
```

### (1) Divide the data

Please check the appendix for the code 

```{r 2.1,echo=FALSE}
######################  Assignment 2.1 #########################################


# Read data
data <- read.csv("bank-full.csv",header = TRUE, sep = ";")

# remove the duration column
data <- data %>% select(-duration)

# convert categorical variables to factors
data <- data %>% mutate_if(is.character, as.factor)

row_num <- nrow(data)
cols_num <- ncol(data)

# Set data split ratio to 0.4, 0.3, 0.3
ratio <- c(train = .4, validate = 0.3, test = .3)

# Set random seed
set.seed(12345)

# Split data to train, validate and test
train_id <- sample(1:row_num, floor(row_num * ratio[1]))
train_set <- data[train_id, ]

# Set random seed
set.seed(12345)

validation_test_id <- setdiff(1:row_num, train_id)
validation_id <- sample(validation_test_id, floor(row_num * ratio[2]))
validation_set <- data[validation_id, ]

test_id <- setdiff(validation_test_id, validation_id)
test_set <- data[test_id, ]
```

### (2)

The following is the default tree.

```{r 2.2.1,echo=FALSE,out.height="150px", fig.align = 'center'}
######################  Assignment 2.2 #########################################

# default fit
tree_a <- tree(y ~ ., data = train_set)
plot(tree_a,type = "uniform",main = "default fit")
```

Following is the tree with minimal node size = 7000.

```{r 2.2.2,echo=FALSE,out.height="150px",fig.align = 'center'}
# min node size = 7000
tree_b <- tree(y ~ ., 
               data = train_set, 
               control = tree.control(nobs = nrow(train_set), minsize = 7000), 
               split = c("deviance","gini"))
plot(tree_b,type = "uniform",main = "min node size = 7000")
```

Following is the tree with mindev = 0.0005.

```{r 2.2.3,echo=FALSE,out.height="150px",fig.align = 'center'}
# min deviance = 0.0005
tree_c <- tree(y ~ ., 
               data = train_set, 
               control = tree.control(nobs = nrow(train_set), mindev = 0.0005), 
               split = c("deviance","gini"))
plot(tree_c,type = "uniform",main = "min deviance = 0.0005")

```

```{r 2.2.4, echo=FALSE}
calc_misclassification_error <- function(A,B){
  return(1-sum(diag(table(A,B) ))/ length(A))
}

calc_misclassification_rate <- function(tree_model,dataset){
  predictions <- predict(object = tree_model, newdata=dataset)
  
  maxIndex <- apply(predictions, 1, which.max)
  prediction_tree_calc <- levels(dataset$y)[maxIndex] 
  data_set_misclassification_rate <- calc_misclassification_error(dataset$y,prediction_tree_calc)
  info <- paste("misclass rate of ",as.character(substitute(tree_model))," and ", 
             as.character(substitute(dataset)) ," is ",data_set_misclassification_rate,"")
  misclassifications <<- c(misclassifications,info)
}

misclassifications <- c()

calc_misclassification_rate(tree_a,train_set)
calc_misclassification_rate(tree_a,validation_set)
calc_misclassification_rate(tree_b,train_set)
calc_misclassification_rate(tree_b,validation_set)
calc_misclassification_rate(tree_c,train_set)
calc_misclassification_rate(tree_c,validation_set)

for (i in 1:6){
  print(misclassifications[i])
}

```
Misclassifications for different trees and datasets are listed above.

According to the misclassification rates and the plots, we can say that Tree B is the best choice, it has the same value as Tree A but is simpler than Tree A and much simpler than Tree C.

According to the information given, we know that a large node size allows leaf nodes to have more observations which will lead to fewer splits, resulting in a smaller tree. 

A smaller deviance, however, allows the tree to be more flexible which leads to trees with many branches and leaves.


### (3)

```{r 2.3.1,echo=FALSE, cache=TRUE,fig.align = 'center'}
optimal_depth <- cv.tree(tree_c)

training_score<- rep(1,50)
validation_score <- rep(1,50)

prune_df <- data.frame(matrix(ncol = 3,nrow = 0))
colnames(prune_df) <- c("Leaves","Train_Deviance","Validation_Deviance")

# from 2 because 'predict' will have an error complaining about applied 
# to an object of class "singlenode"
for(i in 2:50){
  opt_tree_prune <- prune.tree(tree_c,best = i)
  pred_valid <- predict(opt_tree_prune,
                        newdata = validation_set,
                        type = "tree")

  training_score[i] <- deviance(opt_tree_prune)
  validation_score[i] <- deviance(pred_valid)                      

  prune_df <- rbind(  
                      prune_df,
                      data.frame(
                        'Leaves' = i,
                        'Train_Deviance' = training_score[i],
                        'Validation_Deviance' = validation_score[i] 
                      )
                    )
}

# plot the dependence of deviances on training and validation data vs the number of leaves
ggplot(data = prune_df, aes(x = Leaves)) +
  geom_point(aes(y = Train_Deviance, color = "Training Deviance"), size = 1) +
  geom_point(aes(y = Validation_Deviance, color = "Validation Deviance"), size = 1) +
  labs(title = "Dependence of Deviances vs the Number of Leaves",
       x = "Number of Leaves",
       y = "Deviance") +
  scale_color_discrete(name="") 

```

The above is the plot for the number of leaves vs deviance of training and validation data.

From the graph, we can see that when the number of leaves grows, the training deviance decreases, while the validation deviance decreases but then at around 20, it begins to grow.

That means on the left side of that point, the model training and validation data both fit well, but on the right side of that point, the validation data does not fit well.

```{r 2.3.2,echo=FALSE}
optimal_leaves <- prune_df[which.min(prune_df$Validation_Deviance),1]
```

The optimal point according to the output of the code, is `r optimal_leaves`.

We draw the tree based on the optimal number of leaves `r optimal_leaves`, followed by a summary of this tree.

```{r 2.3.4,echo=FALSE,fig.align = 'center'}
opt_tree_with_optimal_leaves <- prune.tree(tree_c,best = optimal_leaves) 
plot(opt_tree_with_optimal_leaves,type = "uniform",main = "tree with optimal leaves")
summary(opt_tree_with_optimal_leaves)
```
According to the output, we know that the variances that are very important in the decision tree are:

$\textbf{poutcome, month, contact, pdays, age, day, balance, housing, job}$


### (4)

We calculate the 
```{r 2.4.1,eval=FALSE}
######################  Assignment 2.4 #########################################

```

```{r 2.4.2,eval=FALSE, cache=TRUE,}
validation_predictions <- predict(final_tree, validation_set, type = "class")

conf_matrix <- confusionMatrix(validation_predictions, validation_set$y)

accuracy <- conf_matrix$overall["Accuracy"]
f1_score <- conf_matrix$byClass["F1"]

# Print the confusion matrix, accuracy, and F1 score
cat("Confusion Matrix:\n")
cat(conf_matrix$table,"\n")

cat("Accuracy:", accuracy, "\n")
cat("F1 Score:", f1_score, "\n")
```

### (5)

```{r 2.5, cache=TRUE,eval=FALSE}
######################  Assignment 2.5 #########################################
loss_matrix <- matrix(c(0, 1, 5, 0), nrow = 2)
test_predictions <- predict(final_tree, test_set, type = "class", parms = list(loss = loss_matrix))
conf_matrix <- table(Actual = test_set$y, Predicted = test_predictions)
cat("Confusion Matrix:\n")
print(conf_matrix)

```

### (6)
```{r 2.6,eval=FALSE, cache=TRUE,eval=FALSE}
######################  Assignment 2.6 #########################################
threshold <- 0.05  
logistic_model <- glm(y ~ ., data = train_set, family = "binomial")

logistic_probabilities <- predict(logistic_model, test_set, type = "response")
logistic_predictions <- ifelse(logistic_probabilities > threshold, "yes", "no")

conf_matrix_logistic <- confusionMatrix(logistic_predictions, test_set$y)

TP <- conf_matrix_logistic$table["yes", "yes"]
FN <- conf_matrix_logistic$table["no", "yes"]
FP <- conf_matrix_logistic$table["yes", "no"]
TN <- conf_matrix_logistic$table["no", "no"]

TPR <- TP / (TP + FN)
FPR <- FP / (FP + TN)

# Print the results
cat("True Positive Rate (TPR):", TPR, "\n")
cat("False Positive Rate (FPR):", FPR, "\n")
```

## Assignment 3. Principal components and implicit regularization

### Answer:

```{r setup3, include=FALSE, cache=TRUE}
###########################  Init code For  Assignment 3 #######################
rm(list=ls(all.names = T))
library(ggplot2)
library(caret)
set.seed(12345)
data <- read.csv("communities.csv")
```

### (1)
According to the output, we need 35 components to obtain at least 95% of variance in the data. The proportion of variation explained by first and second principal components are 0.2501699 and 0.1693597 respectively.

```{r 3.1, cache=TRUE}
######################  Assignment 3.1 #########################################
n <- nrow(data)
features <- data[, -101]

s_features <- scale(features)

S <- (t(s_features) %*% s_features)/n  # sample covariance matrix

Eig <- eigen(S) 

# eigen in descending order
s_indx <- order(Eig$values, decreasing = TRUE)
s_eig <- Eig$values[s_indx]

# cumulative explained variance
cum_var <- cumsum(s_eig) / sum(s_eig)

q_95 <- which(cum_var >= 0.95)[1]  # q for 95% var

first_two_components <- Eig$vectors[, 1:2]  # first two PC

# proportion of variation explained by each of the first two components
PC1_var <- s_eig[1] / sum(s_eig)
PC2_var <- s_eig[2] / sum(s_eig)

cat("Number of components needed for 95% variance:", q_95, "
Proportion of variation explained by the first component:", PC1_var, "
Proportion of variation explained by the second component:", PC2_var, "\n")
```

### (2)

We observe from the plot how the variable "state", the first feature of PC1, 
explains most of the data. The other variables carry significantly less 
explanation. The other 4 features that contribute the most are features 93 
(PctBornSameState), 61 (PctSpeakEnglOnly), 5 (racePctWhite) and 76 
(PersPerRentOccHous).

```{r 3.2, cache=TRUE}
######################  Assignment 3.2 #########################################
PCA <- princomp(features)
L <- PCA$loadings
plot(L[,1], xlim=c(0,20))
highest5 <- order(L[,1], decreasing=TRUE)[1:5]
plot(L[,1], L[,2], main="PCA Scores")
```

### (3)

Train MSE is 0.2752071 and Test MES is 0.4248011.
We observe how the MSE of the test data is significantly bigger than
the MSE obtained from train data. This might happen because the model chosen is overfitting our given data.

```{r 3.3, cache=TRUE}
######################  Assignment 3.3 #########################################
# train and test data
id <- sample(1:n, floor(n*0.5))
trn <- data[id,]
tst <- data[-id,]

# scaling
scaler <- preProcess(trn)
trainS <- predict(scaler,trn)
testS <- predict(scaler,tst)

# linear regression model and test data predictions
linmod <- lm(trainS$ViolentCrimesPerPop ~ ., trainS)
test_pred <- predict(linmod, testS[,-101])

# training and test data MSE
train_MSE <- mean((trainS$ViolentCrimesPerPop - linmod$fitted.values)^2)
test_MSE <- mean((testS$ViolentCrimesPerPop - test_pred)^2)

cat("Train mean squared error:", train_MSE, "\nTest mean squared error:", test_MSE)

```

### (4)

Looking at the plots we can see how the errors converge to zero
after 1700 iterations (approx 1200 in the second graph but considering that we took off
the first 500). Hence 1700 is the optimal iteration number to get good results.
The following iterations do not significally improve our model
and hence can led us to overfitting.


```{r 3.4, cache=TRUE}
######################  Assignment 3.4 #########################################
train_new <- as.matrix(trainS[,-101])  # training data - response variable
train_r <- trainS[,101] # training response variable

test_new <- as.matrix(testS[,-101])  # test data - response variable
test_r <- trainS[,101] # test response variable

# error vectors for training and test data
train_e <- c()
test_e <- c()

costfun <- function(theta_vec){
  train_cost <- mean(((train_new %*% theta_vec) - train_r)^2)
  train_e <<- c(train_e, train_cost)
  test_cost <- mean(((test_new %*% theta_vec) - test_r)^2)
  test_e <<- c(test_e, test_cost)
  return(train_cost)
}

theta0 <- rep(0,100)
opt <- optim(theta0, method="BFGS", costfun)

opt$val

par(mfrow=c(1,2))

plot(ylab="errors", xlab="iteration",train_e, type = "l", main="Errors")
lines(test_e, col="red")
legend("topright", legend = c("Train errors", "Test errors"), 
col = c("black", "red"), lty = 1, cex = 0.5)

plot(ylab="errors", xlab="iteration",train_e[-c(1:500)][1:2000], 
type = "l", main="Zoom on errors")
lines(test_e[-c(1:500)][1:2000], col="red")
legend("topright", legend = c("Train errors", "Test errors"), 
col = c("black", "red"), lty = 1, cex = 0.5)

```



\newpage
# Appendix: All code for this report

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```
