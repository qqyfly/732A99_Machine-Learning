---
title: "Machine Learning Computer Lab 1 (Group A7)"
author: 
  - Qinyuan Qi(qinqi464)
  - Satya Sai Naga Jaya Koushik	Pilla (satpi345)
  - Daniele	Bozzoli(danbo826)  
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup1, include=FALSE}
###########################  Init code For Assignment 1 ########################
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
library(kknn)
library(ggplot2)
library(Metrics)
library(caret)


```

## Assignment 1: Handwritten digit recognition with K- nearest neighbors  (Solved by Qinyuan Qi - qinqi464)

### Answer:

### (1)

The following code will import the data and divide the data into training, validation and test sets.

```{r 1.1}
######################  Assignment 1.1 #########################################
# read data
data <- read.csv("optdigits.csv")
row_num <- nrow(data)
cols_num <- ncol(data)

# set the last column name as "label_value" since we need to create a formula later
names(data)[cols_num] <- "label_value"

# set data split ratio to 0.5, 0.25 and 0.25
ratio <- c(train = .5, validate = .25, test = .25)

# data pre-processing
# columns 1-64 are number based and do not need to be normalized, last column
# is integer represent number from 0-9 which don't need to process again

# set random seed
set.seed(12345)

# split data to training, test and validation set
train_id <- sample(1:row_num, floor(row_num * ratio[1]))
train_set <- data[train_id, ]

set.seed(12345)
test_val_id <- setdiff(1:row_num, train_id)
valid_id <- sample(test_val_id, floor(row_num * ratio[2]))
valid_set <- data[valid_id, ]

test_id <- setdiff(test_val_id, valid_id)
test_set <- data[test_id, ]
```

### (2)

The code contain a function call to kknn with k=30, kernel = ”rectangular” with distance default to 2. We calculate the confusion matrices and the misclassification errors of training and test data set as follows, the code attached in the appendix.
As the data showed below, the overall prediction quality is not good enough when k=30. And the misclassification rate of number 0,6 is the lowest. number 8 and 9 have the highest misclassification rate.

```{r 1.2,echo = FALSE}
######################  Assignment 1.2 #########################################

k_value <- 30
kknn_train <- kknn(label_value ~ .,
                         train_set,
                         train_set,
                         k = k_value,
                         kernel = "rectangular",scale = TRUE)

# generate confusion matrix for training data
confusion_matrices_train <- table(round(kknn_train$fit), train_set$label_value)

# print confusion matrix
#print(confusion_matrices_train)


# calculate error rate
error_rate_train <- mean(round(kknn_train$fit) != train_set$label_value)
#cat("Misclassification error for training data is: ", error_rate_train, "\n")

kknn_valid <- kknn(label_value ~ .,
                  train_set,
                  valid_set,
                  k = k_value,
                  kernel = "rectangular",scale = TRUE)

# generate confusion matrix for validate data
confusion_matrices_valid <- table(round(kknn_valid$fit), valid_set$label_value)


# print confusion matrix
#print(confusion_matrices_valid)

# calculate error rate
error_rate_valid <- mean(round(kknn_valid$fit) != valid_set$label_value)
#cat("Misclassification error for valid data is: ", error_rate_valid, "\n")

# misclassification rates
misclassification_rates_data_1_2 <- data.frame(name=c("Training","Validation"),
                                               rate=c(error_rate_train,
                                                      error_rate_valid))
names(misclassification_rates_data_1_2)[1] <- ""                                                    

# misclassification errors for each digits on training data
misclassification_rates_for_each_digits_data_1_2 <- data.frame(c(0,9),rate=rep(0,10))
names(misclassification_rates_for_each_digits_data_1_2)[1] <- ""

for(i in 1:10){
  misclassification_rates_for_each_digits_data_1_2[i,] <- c(i-1,mean(
    round(kknn_train$fit) != train_set$label_value & 
    train_set$label_value==(i-1)))
}


# render result to tables

knitr::kable(confusion_matrices_train, 
             caption = "Confusion matrix for training data set")
knitr::kable(confusion_matrices_valid,
             caption = "Confusion matrix for validation data set")
knitr::kable(misclassification_rates_data_1_2,
             caption = "Misclassification rates")
knitr::kable(misclassification_rates_for_each_digits_data_1_2,
             caption = "Misclassification rates of digits using learning data set")
```

### (3)

By using image function, we generate first 64 [8 image] from learning data set, and we get the following plot.(We use image function instead of heatmap so we can draw more images in one plot)

According to the plot, we can find that image at [3,2] and [5,2] are easy to identify. however, images such as [3,4] [4,2] [4,3] are very hard to figure out the number.

```{r 1.3,echo = FALSE}
######################  Assignment 1.3 #########################################

# Find all the 8 images
train_eight <- train_set[train_set[,65]==8, 1:64]

# draw 64 [8 images]
par(mfrow = c(8, 8), mar=c(1,1,1,1))

for(i in 1:64){
  row.id <- i
  m <- matrix(as.numeric(train_eight[row.id, 1:64]), nrow=8, ncol=8)
  image(m[,8:1], col=grey(seq(0, 1, length=16)))
}

```

### (4)

Using similar code as in 1.2, we fit the knn with k = 1 to 30. 

When K increase, since all the points are related to more neighbors, we can say that the model become more complex. misclassification rates also increase. And validation data's misclassification rates always greater than train data's misclassification rates.

According to the plot, we know that optimized K = 1 which will generate the smallest misclassification rate.

When K = 1, we training 3 models, we got the following result in table 5.
We can get the conclusion that the test error rate is higher than the validation error rate.
Both validation error rate and test error rate are greater than the value we get from the training data set.

```{r 1.4,echo = FALSE}
######################  Assignment 1.4 #########################################
# calculate error rate for different k values
error_rates_train <- c()
error_rates_valid <- c()

for(k_value in 1:30){
 
  kknn_train <- kknn(label_value ~ .,
                           train_set,
                           train_set,
                           k = k_value,
                           kernel = "rectangular",scale = TRUE)

  kknn_valid <- kknn(label_value ~ .,
                          train_set,
                          valid_set,
                          k = k_value,
                          kernel = "rectangular",scale = TRUE)
  
  # train error rate
  error_rates_train <- append(error_rates_train,
                        mean(round(predict(kknn_train)) != train_set$label_value))


  # valid error rate
  error_rates_valid <- append(error_rates_valid,
                        mean(round(predict(kknn_valid)) != valid_set$label_value))

  
}

# plot the error rate graph
k <- 1:30

error_rate_data <- data.frame(k, error_rates_train,error_rates_valid)

ggplot(data = error_rate_data) +
  geom_line(mapping = aes(x=k,y=error_rates_train,colour="train")) +  
  geom_line(mapping = aes(x=k,y=error_rates_valid,colour="validation")) +
  labs(x = "K",y="Misclassification rates") +
  scale_color_manual(name = "Data set", values = c("train" = "blue", "validation" = "red"))


k_value <- 1

kknn_train <- kknn(label_value ~ .,
                           train_set,
                           train_set,
                           k = k_value,
                           kernel = "rectangular",scale = TRUE)

kknn_valid <- kknn(label_value ~ .,
                          train_set,
                          valid_set,
                          k = k_value,
                          kernel = "rectangular",scale = TRUE)

kknn_test <- kknn(label_value ~ .,
                          train_set,
                          test_set,
                          k = k_value,
                          kernel = "rectangular",scale = TRUE)
  
# train error rate
error_rate_train <- mean(round(predict(kknn_train)) != train_set$label_value)


# valid error rate
error_rate_valid <- mean(round(predict(kknn_valid)) != valid_set$label_value)

# test error rate
error_rate_test <- mean(round(predict(kknn_test)) != test_set$label_value)

data1_4 <- data.frame(c("Training","Validation","Test"),
                      c(error_rate_train,error_rate_valid,error_rate_test))
names(data1_4)[1] <- ""
names(data1_4)[2] <- "Test Error"
knitr::kable(data1_4,
             caption = "Misclassification rates of when K=1")

```

### (5)

We plot the validation error using the code attached in the appendix, the plot of validation error and K as follows.

Since when K=2 get the minimal value, so the optimal K = 2.

The reason why we use cross-entropy is more suitable is:
1) It is sensitive to the predicted probabilities.
2) In multinomial distribution, the predicted value will more likely to be explained as a probability, and cross-entropy loss function is designed to compare the probability based prediction.


```{r 1.5,echo = FALSE}
######################  Assignment 1.5 #########################################
# init value

k_values <- 1:30
epsilon <- 1e-15
validation_errors <- numeric(length(k_values))

for(k in k_values){
    kknn_train <- kknn(label_value ~ .,
                           train_set,
                           valid_set,
                           k = k,
                           kernel = "rectangular",scale = TRUE)

    pred <- round(predict(kknn_train))
    validation_errors[k] <- -sum(valid_set$label_value * log(pred + epsilon))
}

error_cross_entropy_data <- data.frame(k_values,validation_errors)

ggplot(data = error_cross_entropy_data) +
  geom_line(mapping = aes(x=k_values,y=validation_errors))  +
  labs(x = "K",y="Cross entropy error") 

```

## Assignment 2: Linear regression and ridge regressions (Solved by Satya Sai Naga Jaya Koushik Pilla)

### Answer:


```{r setup2, include=FALSE, eval=FALSE}
###########################  Init code For Assignment 2 ########################
rm(list = ls())
library(caret)
knitr::opts_chunk$set(echo = TRUE)

```

### (1)

We read the data and divide data into training and test data (60/40) and normalize them.
```{r 2.1}
###########################  Assignment 2.1 ####################################
# Load the data
data <- read.csv("parkinsons.csv")

# remove useless data
data <- data[,-(1:4)]
data <- data[,-(2)]

row_num <- nrow(data)
cols_num <- ncol(data)

# Divide the data into training and test data (60/40)
# set data split ratio
ratio <- c(train = .6, test = .4)

# set random seed
set.seed(12345)

# split data
train_id <- sample(1:row_num, floor(row_num * ratio[1]))
train_set <- data[train_id, ]
test_id <- setdiff(1:row_num, train_id)
test_set <- data[test_id, ]

# scale data.
preProcValues <- preProcess(train_set, method = c("scale"))
train_set <- predict(preProcValues,train_set)
test_set <- predict(preProcValues,test_set)

```

### (2)


We create a linear model, we get the output as show below.

We have test MSE = 0.9354477.

According to the output of the model, we also know that Jitter.RAP,Jitter.DDP,Shimmer.APQ3,Shimmer.DDA contribute significantly to the model.

```{r 2.2,echo = FALSE}
###########################  Assignment 2.2 ####################################
# Linear regression model

# apply linear regression(we already removed the data we don't need)
model <- lm(motor_UPDRS ~ ., data = train_set)

# predict the test value using the linear regression just created
test_pred <- predict(model, test_set)

# calculate test MSE
test_mse <- mean((test_pred - test_set$motor_UPDRS)^2)
cat("test mse is " , test_mse , "\n")
model
```

### (3)

Functions implemented as below.
```{r 2.3}
###########################  Assignment 2.3 ####################################
# Loglikelihood function
loglikelihood <- function(x,y, theta, sigma) {
  n <- length(x)
  log_likelihood_value <- -0.5 * (n * log(2 * pi * sigma^2)) -
                          sum((t(theta) * x - y)^2) / (2 * sigma^2)
  return(log_likelihood_value)
}

# ridge
ridge <- function(par,x,y,lambda) {

  param_length <- length(par)

  theta <- par[1:param_length - 1]
  sigma <- par[param_length]

  loglikelihood_result <- loglikelihood(x,y,theta, sigma)
  ridge_penalty <- lambda * sum(theta^2)
  return(loglikelihood_result - ridge_penalty)
}

# ridgeopt
ridgeopt <- function(initial_values, x,y,lambda ) {
  # Use optim function
  
  result <- optim(par = initial_values, ridge, method = "BFGS",
                  x = x, y = y,lambda = lambda)
  return(result)
}

# df
df <- function(x, y, theta, sigma) {
  hat_y <- t(theta) %*% x
  return(sum(cov(hat_y, y)) / sigma^2)
}

```

### (4)

Functions implemented as below.
```{r 2.4}
###########################  Assignment 2.4 ####################################

x <- train_set[,-1]
y <- train_set$motor_UPDRS
colnum_num <- ncol(x)

#lambda <- 1
theta <- rep(1, ncol(x))
sigma <- 1
initial_values <- c(theta, sigma)
result1 <- ridgeopt(initial_values=initial_values, x = x, y=y,lambda = 1 )
result1
#train_pred1 <- t(train_set[,-1]) %*% result1$par[,1:colnum_num-1] + result1$par[,colnum_num]
#train_mse1 <- mean((train_pred1 - train_set$motor_UPDRS)^2)
#test_pred1 <- test_set[,-1] %*% result1$par[,1:colnum_num-1] + result1$par[,colnum_num]
#test_mse1 <- mean((test_pred1 - test_set$motor_UPDRS)^2)
#df1 <- df(x,y,result1$par[,1:colnum_num-1],result1$par[,colnum_num])



#lambda <- 100
theta <- rep(1, ncol(x))
sigma <- 1
initial_values <- c(theta, sigma)
result100 <- ridgeopt(initial_values=initial_values, x = x, y=y,lambda = 100 )
result100
#train_pred100 <- train_set[,-1] %*% result100$par[,1:colnum_num-1] + result100$par[,colnum_num]
#train_mse100 <- mean((train_pred100 - train_set$motor_UPDRS)^2)
#test_pred100 <- test_set[,-1] %*% result100$par[,1:colnum_num-1] + result100$par[,colnum_num]
#test_mse100 <- mean((test_pred100 - test_set$motor_UPDRS)^2)
#df100 <- df(x,y,result100$par[,1:colnum_num-1],result100$par[,colnum_num])

#lambda <- 1000
theta <- rep(1, ncol(x))
sigma <- 1
initial_values <- c(theta, sigma)
result1000 <- ridgeopt(initial_values=initial_values, x = x, y=y,lambda = 1000 )
result1000

#train_pred1000 <- train_set[,-1] %*% result1000$par[,1:colnum_num-1] + result1000$par[,colnum_num]
#train_mse1000 <- mean((train_pred1000 - train_set$motor_UPDRS)^2)
#test_pred1000 <- test_set[,-1] %*% result1000$par[,1:colnum_num-1] + result1000$par[,colnum_num]
#test_mse1000 <- mean((test_pred1000 - test_set$motor_UPDRS)^2)
#df1000 <- df(x,y,result1000$par[,1:colnum_num-1],result1000$par[,colnum_num])


#data2_4 <- data.frame(c("1","100","1000"),
#                      c(mse1,mse100,mse1000),
#                      c(pred1,pred100,pred1000),
#                      c(df1,df100,df1000))

#names(data2_4)[1] <- "lambda"
#names(data2_4)[2] <- "MSE"
#names(data2_4)[3] <- "Predicted Value"
#names(data2_4)[4] <- "df"

#knitr::kable(data2_4,
#             caption = "Values when lambda=1,100,1000")

```

## Assignment 3. Logistic regression and basis function expansion (Solved by Daniele	Bozzoli)

### Answer:

```{r setup3, include=FALSE, eval=FALSE}
###########################  Init code For Assignment 3 ########################
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
```

### (1)

```{r 3.1,echo = FALSE}
data <- read.csv("pima-indians-diabetes.csv")

age <- data[, 8]
plasma <- data[, 2]

names(data)[9] <- "diabetes"
names(data)[2] <- "plasma"
names(data)[8] <- "age"

x <- age
y <- plasma
df <- data.frame(x, y)

ggplot2::ggplot(df, ggplot2::aes(age, plasma)) +
  ggplot2::geom_point(ggplot2::aes(colour = data[, 9]))+
  ggplot2::labs(x = "Age", y="Plasma glucose")

```

We observe how a lot of people without diabete is highly concentrated in the lower 
part of the age axis, explaining how younger individuals tend to be non-diabetic,
on the other hand, we notice how especially people with higher plasma glucose 
concentration tend to be diabetic, less concentrated in a specific age group, more
spread across the x-age axis.

### (2)(3)

Code as below.
Looking at the graph, we notice how the classification is not very accurate. Though, it 
catches the fact that people with higher plasma glucose concentration are more 
commonly diabetic. We obtain a misclassification error = 0.266.

```{r 3.2_3.3}

model <- glm(diabetes ~ plasma + age, fam = binomial(link="logit"), data = data)
summary(model)

# r = 0.5

c1 <- fitted(model) >= .5

missC_error <- sum(data$diabetes != c1) / nrow(data)
cat("Misclassification error:", missC_error, "\n")

ggplot2::ggplot(df, ggplot2::aes(age, plasma)) +
  ggplot2::geom_point(ggplot2::aes(colour = c1))+
  ggplot2::labs(x = "Age", y="Plasma glucose")     # T = Diabetic, F = Non-diabetic

```


### (4)

It looks like the line that splits the predicted observations into diabetic
and non diabetic into the two cases ( r= 0.2 and r=0.8) has the same slope, but
different intercept. Both cases are pretty bad predictors as 0.2 and 0.8 are
respectively too low and too high to get a good classification for our case.

```{r 3.4,echo = FALSE}

# r = 0.2

c2 <- fitted(model) >= .2

ggplot2::ggplot(df, ggplot2::aes(age, plasma)) +
  ggplot2::geom_point(ggplot2::aes(colour = c2))+
  ggplot2::labs(x = "Age", y="Plasma glucose")

# r = 0.8

c3 <- fitted(model) >= .8

ggplot2::ggplot(df, ggplot2::aes(age, plasma)) +
  ggplot2::geom_point(ggplot2::aes(colour = c3))+
  ggplot2::labs(x = "Age", y="Plasma glucose")

```

### (5)

The outcome of this analysis is better than the first case, as we obtain a 
lower misclassification error than before, and also graphically we notice
how this version captures what we were saying in the first part of the 
analysis, how people with higher plasma glucose concentration are more 
exposed to the risk of having diabete. We also notice how we strangely get a 
misclassificated predicted value in the bottom right corner of the graph.

```{r 3.5, echo = FALSE}

# Adding new variables, logit function with r= 0.5

z1 <- data$plasma^4
z2 <- data$plasma^3 * data$age
z3 <- data$plasma^2 * data$age^2
z4 <- data$plasma^1 * data$age^3
z5 <- data$age^4

newdata <- cbind(data, z1, z2, z3, z4, z5)

newmodel <- glm(diabetes ~ plasma + age + z1 + z2 + z3 + z4 + z5 , family = binomial(link = "logit") , data = newdata)

summary(newmodel)

c4 <- fitted(newmodel) >= .5

new_missC_error <- sum(data$diabetes != c4) / nrow(data)
cat("Misclassification error:", new_missC_error, "\n")

ggplot2::ggplot(df, ggplot2::aes(age, plasma)) +
  ggplot2::geom_point(ggplot2::aes(colour = c4))+
  ggplot2::labs(x = "Age", y="Plasma glucose")

```




\newpage
# Appendix: All code for this report

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```
