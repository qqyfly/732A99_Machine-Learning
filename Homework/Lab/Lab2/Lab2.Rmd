---
title: "Machine Learning Computer Lab 2 (Group A7)"
author: 
  - Qinyuan Qi(qinqi464)
  - Satya Sai Naga Jaya Koushik	Pilla (satpi345)
  - Daniele	Bozzoli(danbo826)  
date: "`r Sys.Date()`"
output: pdf_document
---

## Statement of Contribution
For Lab 2, we decided to split the three assignments equally, Qinyuan completed Assignment 1, Satya
completed Assignment 2 and Daniele completed Assignment 3, after which, for verification's sake, we completed
each other's assignments as well and validated our findings. The report was also compiled by three of us,
with each handling their respective assignments.


## Assignment 1: Explicit regularization
### Answer:

```{r setup1, echo=FALSE,warnings=FALSE}
###########################  Init code For  Assignment 1 #######################
rm(list = ls())
library(plyr)
library(readr)
library(dplyr)
library(caret)
library(ggplot2)
library(repr)
library(glmnet)
library(tree)
library(gridExtra)

# set random seed
set.seed(12345)
```

### (1)

According to the question, we can create a linear model like the following.

$$
Fat = \beta_{0} + \beta_{1} Channel_{1} + \beta_{2}Channel_{2}+ ... + \beta_{100}Channel_{100} + \epsilon = \sum{\beta_{i} Channel_{i}} + \epsilon
$$

In the above formula, $\beta_{0}$ is the intercept, while the remaining $\beta$ are the parameters corresponding to each channel.

According to the output, the model generated uses 100 channel features, and almost all the channels provide contributions to the target.
However, the p-value shows that only very limited channels are useful. And the  $MSE_{test} =  722.4294$ , $MSE_{training} = 0.005709117$. 
It means the training data fit pretty well, however, the test data fit not as expected, and the model overfits the data.

```{r 1.1, echo=FALSE, cache=TRUE}
######################  Assignment 1.1 #########################################


# read data
data <- read.csv("tecator.csv")
data <- data %>% select(Fat,Channel1:Channel100)

row_num <- nrow(data)
# set data split ratio to 0.5, 0.5
ratio <- c(train = .5, test = .5)

# split data to the train dataset and the test dataset
train_id <- sample(1:row_num, floor(row_num * ratio[1]))
train_data_set <- data[train_id, ]

test_id <- setdiff(1:row_num, train_id)
test_data_set <- data[test_id, ]

# fit linear model
lm_model <- lm(Fat ~ ., data = train_data_set)

predicted_fat_test <- predict(lm_model, test_data_set)
predicted_fat_train <- predict(lm_model, train_data_set)

# calc the mean value
test_mse <- mean((predicted_fat_test - test_data_set$Fat)^2) 
train_mse <- mean((predicted_fat_train - train_data_set$Fat)^2) 
cat("test_mse is:",test_mse,"\n")
cat("train_mse is:",train_mse,"\n")

summary(lm_model)
```

### (2)
The cost function in lasso regression is:

$$
\hat{\theta} = argmin_{\theta} \frac{1}{n} || X\theta||^{2} + \lambda ||\theta||_{1}
$$

### (3)

According to the plot of lasso_reg, we find that when $log(\lambda)$ is small, the degree of freedom is big which
means more features explain the target variable, but when $log(\lambda)$ increases, the degree of freedom decrease 
at the same time, and coefficient get close to 0 in most of the time. We also find that some of the feature's 
coefficient line fluctuates when it is less than 0. But all of the features's coefficients will converge to 0 at last.

```{r 1.3.1, echo=FALSE, cache=TRUE}
######################  Assignment 1.3 #########################################
x_train <- as.matrix(train_data_set %>% select(-Fat))
y_train <- as.matrix(train_data_set %>% select(Fat))

# when alpha = 1, it is lasso regression
lasso_reg <- glmnet(x_train, y_train, alpha = 1,family = "gaussian")
```

```{r 1.3.2,echo=FALSE, cache=TRUE}

lambda_deg_freedom <- data.frame(lambda = lasso_reg$lambda,
                                 deg_freedom = lasso_reg$df)

lambda_deg_freedom <- lambda_deg_freedom %>% arrange(desc(deg_freedom))

lambda_deg_freedom_num <- lambda_deg_freedom[which(lambda_deg_freedom$deg_freedom == 4),] %>% 
                        head(1) %>% pull(lambda)

#cat("lambda is ",lambda_deg_freedom_num,
#    " log(lambda) is ",log(lambda_deg_freedom_num),
#    "\nwhen feature number is 4 (including the intercept)\n")
```

According to the calculation and output of the code, we find that when the feature number is 4 (including the intercept),
lambda is: `r lambda_deg_freedom_num` and $log(\lambda)$ is `r log(lambda_deg_freedom_num)` .

```{r 1.3.3,echo=FALSE, cache=TRUE}
plot(lasso_reg, xvar="lambda", xlab='Log Lambda')
abline(v=log(lambda_deg_freedom_num), col="red")
```

### (4)

The cost function in ridge regression is:

$$
\hat{\theta} = argmin_{\theta} \frac{1}{n} || X\theta||^{2} + \lambda ||\theta||_{2}^{2}
$$

For ridge regression, we find that the degree of freedom always keeps the same when $log(\lambda)$ increases,
but the coefficient of features seems to converge to 0, after checking the beta value of ridge_reg, we found 
that the coefficient of features will not converge to 0, it will become a very small value.

Compared to lasso regression, we can not find the required lambda value when the degree of freedom is 4, since the coefficient
value not converge to 0, but converges to a very small value as stated above.

```{r 1.4,echo=FALSE, cache=TRUE}
######################  Assignment 1.4 #########################################
# when alpha = 0, it is ridge regression
ridge_reg <- glmnet(x_train, y_train, alpha = 0,family = "gaussian")

lambda_deg_freedom <- data.frame(lambda = ridge_reg$lambda,
                                 deg_freedom = ridge_reg$df)

lambda_deg_freedom <- lambda_deg_freedom %>% arrange(desc(deg_freedom))

lambda_deg_freedom_num <- lambda_deg_freedom[which(lambda_deg_freedom$deg_freedom == 4),] %>% 
                        head(1) %>% pull(lambda)

plot(ridge_reg,xvar="lambda", xlab='Log Lambda')
```

### (5)
The plot is as follows, when $log(\lambda)$ increases, features explaining the model get fewer, and the model becomes simpler.

From the plot, we also find that when there are at least 9 features, the MSE is very stable and keeps at a lower level. 
However, when $log(\lambda)$ increases, MSE increases rapidly until 3 features. It means those 9 features are significant.

```{r 1.5.1,echo=FALSE,out.height="250px", cache=TRUE,fig.align = 'center'}
# when alpha = 1, it is lasso regression
cv_lasso_model <- cv.glmnet(x_train, y_train, alpha = 1, family = "gaussian") 

# Plot the dependence of CV score on log(lambda)
plot(cv_lasso_model,xlab = "log lambda")
```

```{r 1.5.2,echo=FALSE, cache=TRUE}
# Extract lambda values and CV scores
opt_lambda <- cv_lasso_model$lambda.min
opt_features <- cv_lasso_model$nzero[which(cv_lasso_model$lambda == opt_lambda)]

#cat("Optimal log(lambda) is:",log(opt_lambda),"\n")
#cat("Corresponding features used is:", opt_features,"\n")
```

And optimized $\lambda$ is  `r opt_lambda` , $log(\lambda)$ is `r log(opt_lambda)`, in this case, `r opt_features` features were chosen.

Compare the optimized $\lambda$ and corresponding $log(\lambda)$ with the $log(\lambda) = -4$, according to the plot, it's relatively flat in this range.
So we can say that it does not indicate it's possible for $log(\lambda) = -4$ to get statistical significally better prediction.

To compare the models, a scatter plot was used with different colors. 

As we can see from the plot, the green dots(opt lasso) are closer to the original blue dots than the red dots(linear regression).

This means the linear regression model is not as good as the opt lasso model in this case. 

When Fat(<10) is small, linear regression fits well, when Fat gets bigger, linear regression fit points have a big gap with the original data. 
However, the opt lasso model fits well in all the ranges. So regularization used in lasso helps to improve the model.

```{r 1.5.3,echo=FALSE,fig.align = 'center', cache=TRUE}
x_test <- as.matrix(test_data_set %>% select(-Fat))
y_test <- as.matrix(test_data_set %>% select(Fat))

opt_lasso_model <- glmnet(x_train, y_train, alpha = 1, family = "gaussian", lambda = opt_lambda)
opt_lasso_prediction <- predict(opt_lasso_model, 'response', newx = x_test)

test_len <- length(y_test)
scatter_df = data.frame(
              x = rep(y_test, 3),
              y = c(y_test, predicted_fat_test, opt_lasso_prediction),
              Model = rep(c("Original","Linear Regression","Opt Lasso"), each = test_len)
            )
ggplot(scatter_df, aes(x = x, y = y, color = Model)) +
  geom_point() +
  labs(title = "Scatter Plot with Three Sets of Data", x = "Fat", y = "Predicted Value")  + 
  scale_color_discrete(name="") 
```

## Assignment 2: Decision trees and logistic regression for bank marketing

### Answer:

```{r setup2, include=FALSE, cache=TRUE}
###########################  Init code For  Assignment 1 #######################
rm(list = ls())
```

### (1) Divide the data

Please check the appendix for the code 

```{r 2.1,echo=FALSE, cache=TRUE}
######################  Assignment 2.1 #########################################
# Read data
data <- read.csv("bank-full.csv",header = TRUE, sep = ";")

# remove the duration column
data <- data %>% select(-duration)

# convert categorical variables to factors
data <- data %>% mutate_if(is.character, as.factor)

row_num <- nrow(data)
cols_num <- ncol(data)

# Set data split ratio to 0.4, 0.3, 0.3
ratio <- c(train = .4, validate = 0.3, test = .3)

# Set random seed
set.seed(12345)

# Split data to train, validate and test
train_id <- sample(1:row_num, floor(row_num * ratio[1]))
train_set <- data[train_id, ]

# Set random seed
set.seed(12345)

validation_test_id <- setdiff(1:row_num, train_id)
validation_id <- sample(validation_test_id, floor(row_num * ratio[2]))
validation_set <- data[validation_id, ]

test_id <- setdiff(validation_test_id, validation_id)
test_set <- data[test_id, ]
```

### (2)

The following is the default tree.

```{r 2.2.1,echo=FALSE,out.height="150px", fig.align = 'center', cache=TRUE}
######################  Assignment 2.2 #########################################
# default fit
tree_a <- tree(y ~ ., data = train_set)
plot(tree_a,type = "uniform",main = "default fit")
```

Following is the tree with minimal node size = 7000.

```{r 2.2.2,echo=FALSE,out.height="150px",fig.align = 'center', cache=TRUE}
# min node size = 7000
tree_b <- tree(y ~ ., 
               data = train_set, 
               control = tree.control(nobs = nrow(train_set), minsize = 7000), 
               split = c("deviance","gini"))
plot(tree_b,type = "uniform",main = "min node size = 7000")
```

Following is the tree with mindev = 0.0005.

```{r 2.2.3,echo=FALSE,out.height="150px",fig.align = 'center', cache=TRUE}
# min deviance = 0.0005
tree_c <- tree(y ~ ., 
               data = train_set, 
               control = tree.control(nobs = nrow(train_set), mindev = 0.0005), 
               split = c("deviance","gini"))
plot(tree_c,type = "uniform",main = "min deviance = 0.0005")

```

```{r 2.2.4, echo=FALSE, cache=TRUE}
calc_misclassification_error <- function(A,B){
  return(1-sum(diag(table(A,B) ))/ length(A))
}

calc_misclassification_rate <- function(tree_model,dataset){
  predictions <- predict(object = tree_model, newdata=dataset)
  
  maxIndex <- apply(predictions, 1, which.max)
  prediction_tree_calc <- levels(dataset$y)[maxIndex] 
  data_set_misclassification_rate <- calc_misclassification_error(dataset$y,prediction_tree_calc)
  info <- paste("misclass rate of ",as.character(substitute(tree_model))," and ", 
             as.character(substitute(dataset)) ," is ",data_set_misclassification_rate,"")
  misclassifications <<- c(misclassifications,info)
  return(info)
}

misclassifications <- c()

calc_misclassification_rate(tree_a,train_set)
calc_misclassification_rate(tree_a,validation_set)
calc_misclassification_rate(tree_b,train_set)
calc_misclassification_rate(tree_b,validation_set)
calc_misclassification_rate(tree_c,train_set)
calc_misclassification_rate(tree_c,validation_set)

```
Misclassifications for different trees and datasets are listed above.

According to the misclassification rates and the plots, we can say that Tree B is the best choice, it has the same value as Tree A but is simpler than Tree A and much simpler than Tree C.

According to the information given, we know that a large node size allows leaf nodes to have more observations which will lead to fewer splits, resulting in a smaller tree. 

A smaller deviance, however, allows the tree to be more flexible which leads to trees with many branches and leaves.


### (3)

```{r 2.3.1,echo=FALSE, cache=TRUE,fig.align = 'center', cache=TRUE}
optimal_depth <- cv.tree(tree_c)

training_score<- rep(1,50)
validation_score <- rep(1,50)

prune_df <- data.frame(matrix(ncol = 3,nrow = 0))
colnames(prune_df) <- c("Leaves","Train_Deviance","Validation_Deviance")

# from 2 because 'predict' will have an error complaining about applied 
# to an object of class "singlenode"
for(i in 2:50){
  opt_tree_prune <- prune.tree(tree_c,best = i)
  pred_valid <- predict(opt_tree_prune,
                        newdata = validation_set,
                        type = "tree")

  training_score[i] <- deviance(opt_tree_prune)
  validation_score[i] <- deviance(pred_valid)                      

  prune_df <- rbind(  
                      prune_df,
                      data.frame(
                        'Leaves' = i,
                        'Train_Deviance' = training_score[i],
                        'Validation_Deviance' = validation_score[i] 
                      )
                    )
}

# plot the dependence of deviances on training and validation data vs the number of leaves
ggplot(data = prune_df, aes(x = Leaves)) +
  geom_point(aes(y = Train_Deviance, color = "Training Deviance"), size = 1) +
  geom_point(aes(y = Validation_Deviance, color = "Validation Deviance"), size = 1) +
  labs(title = "Dependence of Deviances vs the Number of Leaves",
       x = "Number of Leaves",
       y = "Deviance") +
  scale_color_discrete(name="") 

```

The above is the plot for the number of leaves vs deviance of training and validation data.

From the graph, we can see that when the number of leaves grows, the training deviance decreases, while the validation deviance decreases but then at around 20, it begins to grow.

That means on the left side of that point, the model training and validation data both fit well, but on the right side of that point, the validation data does not fit well.

```{r 2.3.2,echo=FALSE, cache=TRUE}
optimal_leaves <- prune_df[which.min(prune_df$Validation_Deviance),1]
```

The optimal point according to the output of the code, is `r optimal_leaves`.

We draw the tree based on the optimal number of leaves `r optimal_leaves`, followed by a summary of this tree.

```{r 2.3.3,echo=FALSE,fig.align = 'center', cache=TRUE}
opt_tree_with_optimal_leaves <- prune.tree(tree_c,best = optimal_leaves) 
plot(opt_tree_with_optimal_leaves,type = "uniform",main = "tree with optimal leaves")
summary(opt_tree_with_optimal_leaves)
```
According to the output, we know that the variances that are very important in the decision tree are:

$\textbf{poutcome, month, contact, pdays, age, day, balance, housing, job}$


### (4)

The misclassification rate for test and validation data using the pruned tree is as follows.

```{r 2.4.1,echo=FALSE, cache=TRUE}
######################  Assignment 2.4 #########################################
opt_pred_validation <- predict(opt_tree_with_optimal_leaves,
                         newdata = validation_set,
                         type = "class")
misclassifications_opt_tree_with_optimal_leaves_valid <- 
                    calc_misclassification_rate(opt_tree_with_optimal_leaves,validation_set)

opt_pred_test <- predict(opt_tree_with_optimal_leaves,
                         newdata = test_set,
                         type = "class")

misclassifications_opt_tree_with_optimal_leaves_test <- 
                    calc_misclassification_rate(opt_tree_with_optimal_leaves,test_set)

cat(misclassifications_opt_tree_with_optimal_leaves_test)
cat(misclassifications_opt_tree_with_optimal_leaves_valid)
```

The confusion matrix for the test data is as follows.

```{r 2.4.2,echo=FALSE, cache=TRUE}
######################  Assignment 2.4 #########################################
table(test_set$y, opt_pred_test)
```
Accuracy and F1 score for the test data are as follows.

```{r 2.4.3,echo=FALSE, cache=TRUE}
# we constructed a data frame to store all the info we needed
model_matrix_df <- data.frame(matrix(ncol = 4,nrow = 0))
colnames(model_matrix_df) <- c("recall","precision","accuracy","f1_score")

calc_model_matrix <- function(real_val, pred_val){
  confusion_matrix <- table(real_val, pred_val)
  TN <- confusion_matrix[1,1]
  FP <- confusion_matrix[1,2]
  FN <- confusion_matrix[2,1]
  TP <- confusion_matrix[2,2]
  N <- TN + FP
  P <- FN + TP
  TPR <- TP / P
  FPR <- FP / N
  recall <- TP / (TP + FN)
  precision <- TP / (TP + FP)
  accuracy <- (TP + TN) / (N + P)
  f1_score <- 2 * (precision * recall) / (precision + recall)
  model_matrix_df <<- rbind(  
                      model_matrix_df,
                      data.frame(
                        'recall' = recall,
                        'precision' = precision,
                        'accuracy' = accuracy,
                        'f1_score' = f1_score
                      )
                    )
  return(model_matrix_df)
}

calc_model_matrix(test_set$y, opt_pred_test)[1,]
```
According to the output, we know that the accuracy is `r model_matrix_df[1,3]` and the F1 score is `r model_matrix_df[1,4]`.
Comparing it to the results of the validation data, we can say that the model gets a very good prediction result.

It's better to choose the F1 score as the metric to evaluate the model because f1_score involves both precision and recall in its formula 
and those 2 metrics consider more on TP value, so improving f1_score can make the model good at predicting the positive class.

### (5)

We will perform a decision tree classification with a loss matrix by changing the type of the function call of precidt to vector. 
The following is the result.

```{r 2.5,echo=FALSE, cache=TRUE}
######################  Assignment 2.5 #########################################
opt_pred_test_new_loss <- predict(opt_tree_with_optimal_leaves,
                                  newdata = test_set,
                                  type = "vector")


new_test_set <- cbind(test_set, as.data.frame(opt_pred_test_new_loss))
true_label <- new_test_set$y
new_prediction <- c()

# We will change the condition based on the given loss matrix

for(i in 1:length(true_label)){
  if(new_test_set[i, "no"] / new_test_set[i, "yes"] > 5){
    new_prediction <- rbind(new_prediction, "no")
  }else{
    new_prediction <- rbind(new_prediction, "yes")
  }
}

# Output confusion matrix
table(test_set$y, new_prediction)
cat("model misclassification is \n")
calc_misclassification_error(test_set$y, new_prediction)
cat("model matrics are \n")
calc_model_matrix(test_set$y, new_prediction)[2,]
```

We found that f1_score doubled from 0.224554 to 0.4862605. This is because we change the imbalance of FP and TN values.
Which means we add more penalties on TN. Meanwhile, the model's accuracy decreased from 0.8910351 to 0.8731937, just about a 2% decrease.
Since we prefer using f1_score as the metric, we can say that the model is improved.

### (6)

We perform the classification using the optimized tree and logistic regression using the following principle with $\pi = 0.05,0.1,\ldots,0.95$.

$$
\hat{Y} = yes \: if \: p(Y = 'yes'|X) > \pi, \: otherwise \: \hat{Y} = no
$$

```{r 2.6,echo=FALSE, cache=TRUE}
######################  Assignment 2.6 #########################################
opt_tree_pi <- prune.tree(tree = tree_c,
                          best = optimal_leaves,
                          method = "misclass")

log_model_pi <- glm(y ~ ., 
                    data = train_set, 
                    family = binomial)

model_pi_df <- data.frame(matrix(ncol = 5,nrow = 0))
colnames(model_pi_df) <- c("pi","TPR","FPR","Precision","Recall")

calc_pi_tree_func <- function(pi_models, pi_values){
  new_prediction_pi <- c()
  
  opt_pred_test_pi <- as.data.frame(predict(opt_tree_pi,
                                            newdata = test_set))
  for(i in 1:nrow(opt_pred_test_pi)){
    if(opt_pred_test_pi$yes[i] > pi_values){
      new_prediction_pi <- rbind(new_prediction_pi, "yes")
    }else{
      new_prediction_pi <- rbind(new_prediction_pi, "no")
    }  
  }

  confusion_matrix_pi <- table(test_set$y, new_prediction_pi) 
  TN_pi <- confusion_matrix_pi[1,1]
  TP_pi <- confusion_matrix_pi[2,2]
  FP_pi <- confusion_matrix_pi[1,2]
  FN_pi <- confusion_matrix_pi[2,1]

  N_pi <- TN_pi + FP_pi
  P_pi <- FN_pi + TP_pi

  TPR_pi <- TP_pi / P_pi
  FPR_pi <- FP_pi / N_pi

  precision_pi <- TP_pi / (TP_pi + FP_pi)
  recall_pi <- TP_pi / (TP_pi + FN_pi)

  model_pi_df <<- rbind(  
                      model_pi_df,
                      data.frame(
                        'pi' = pi_values,
                        'TPR' = TPR_pi,
                        'FPR' = FPR_pi,
                        'Precision' = precision_pi,
                        'Recall' = recall_pi
                      )
                    )

  return(model_pi_df)
}

calc_pi_logicreg_func <- function(pi_models, pi_values){
  new_prediction_pi <- c()
  logistic_prod_test_pi <- predict(log_model_pi,
                                   newdata = test_set,
                                   type = "response")
  
  logistic_pred_test_pi <- ifelse (logistic_prod_test_pi > pi_values, "yes", "no") 
  confusion_matrix_pi <- table(test_set$y, logistic_pred_test_pi)

  TN_pi <- confusion_matrix_pi[1,1]
  TP_pi <- confusion_matrix_pi[2,2]
  FP_pi <- confusion_matrix_pi[1,2]
  FN_pi <- confusion_matrix_pi[2,1]

  N_pi <- TN_pi + FP_pi
  P_pi <- FN_pi + TP_pi
  TPR_pi <- TP_pi / P_pi
  FPR_pi <- FP_pi / N_pi
  recall_pi <- TP_pi / (TP_pi + FN_pi)
  precision_pi <- TP_pi / (TP_pi + FP_pi)
  model_pi_df <<- rbind(  
                      model_pi_df,
                      data.frame(
                        'pi' = pi_values,
                        'TPR' = TPR_pi,
                        'FPR' = FPR_pi,
                        'Precision' = precision_pi,
                        'Recall' = recall_pi
                      )
                    )
  return(model_pi_df)
}

pi_values <- seq(0.05,0.95,0.05)

for(i in 1:length(pi_values)){
  calc_pi_tree_func(opt_tree_pi, pi_values[i])
 
}
for(i in 1:length(pi_values)){  
  calc_pi_logicreg_func(log_model_pi, pi_values[i])
}

tree_pi_df <- model_pi_df[1:19,]
logistic_pi_df <- model_pi_df[20:38,]

p1 <- ggplot(tree_pi_df, aes(x = FPR, y = TPR)) +
  geom_line() +  
  labs(title = "ROC curve of opt decision tree",
       x = "False Positive Rate",
       y = "True Positive Rate")
p2 <- ggplot(tree_pi_df, aes(x = Recall, y = Precision)) +
  geom_line() +  
  labs(title = "Precision-Recall curve of opt decision tree",
       x = "Recall",
       y = "Precision")
p3 <- ggplot(logistic_pi_df, aes(x = FPR, y = TPR)) +
  geom_line() +  
  labs(title = "ROC curve of opt logistic regression",
       x = "False Positive Rate",
       y = "True Positive Rate")       
p4 <- ggplot(logistic_pi_df, aes(x = Recall, y = Precision)) +
  geom_line() +  
  labs(title = "Precision-Recall curve of opt logistic regression",
       x = "Recall",
       y = "Precision")

grid.arrange(p1, p2, p3, p4, nrow = 2)

```
We can see from the ROC plots that with a fixed FPR, the higher the TPR, the better the model is.

In this case, bank staff is more interested in the client's subscription to a new product. Thus we know that the dataset may be imbalanced.
Meanwhile, to test some threshold values, the PR curve will provide more information than the ROC curve.
 
## Assignment 3. Principal components and implicit regularization

### Answer:

```{r setup3, include=FALSE}
###########################  Init code For  Assignment 3 #######################
rm(list=ls(all.names = T))
library(ggplot2)
library(caret)
set.seed(12345)
```

### (1)
```{r 3.1, echo=FALSE}
######################  Assignment 3.1 #########################################
data <- read.csv("communities.csv",header = TRUE, sep = ",")

features <- data[, -101]
features.scale <- scale(features)

eig <- eigen(cov(features.scale)) # 

# proportion of variation explained by each of the first 100 components

# plot it
plot(eig$values, type = "p", 
     xlab = "Component", 
     ylab = "Proportion of variation explained", 
     main = "Proportion of variation explained by each of the first 100 components")

# cumulative explained variance
cum_var <- cumsum(eig$values)
q_95 <- which(cum_var >= 95)[1]  # q for 95% var

cat("Number of components needed for 95% variance:", q_95, "\n")
cat("Proportion of variation explained by the first component:", eig$values[1] / sum(eig$values), "\n")
cat("Proportion of variation explained by the second component:", eig$values[2] / sum(eig$values), "\n")

```
According to the output, we need 35 components to obtain at least 95% of the variance in the data. 
The proportion of variation explained by the first and second principal components are 0.2501699 and 0.1693597 respectively.


### (2)

```{r 3.2.1, echo=FALSE}
######################  Assignment 3.2 #########################################
PCA <- princomp(features.scale)
res <- PCA$loadings
plot(res[,1], xlim=c(0,100),type = "l",xlab='Features',ylab='PC1',main="Traceplot of 1st principal component")
```
From the plot, we found that some of the features have significant contributions to the first principal component.

The 5 most contributed features are:

```{r 3.2.2, echo=FALSE}
# get the top 5 features that contribute to the first principal component(positive & negative)
sort(abs(res[,1]), decreasing = TRUE)[1:5]
```
medFamInc(median family income): median family income has a relationship with crime low-income parents (especially with kids) would like to get more money to feed their kids

medIncome(median household income): median household income is almost the same as medFamInc, People in those low-income families are more willing to get easy money and most of them are related to crime.

PctKids2Par(percentage of kids in family housing with two parents): kids with two parents are more likely to be well-educated and have a good life, so they are less likely to commit crimes. 

pctWInvInc(percentage of households with investment / rent income in 1989): people with investment/rent income are more likely to be rich, and they are less likely to commit crimes. 

PctPopUnderPov(percentage of people under the poverty level): people under the poverty level are more likely to commit crimes because they need money to survive.


```{r 3.2.3, echo=FALSE}
pc_score_df <- data.frame(PCA$scores)
pc_score_df$ViolentCrimesPerPop <- data$ViolentCrimesPerPop

# plot PC scores
ggplot(data=pc_score_df, aes(x=pc_score_df[,1], y=pc_score_df[,2], color=ViolentCrimesPerPop)) + 
  geom_point(alpha=0.5) + 
  scale_color_gradient(low="blue", high="red") +
  labs(title = "PC1 vs PC2", x = "PC1", y = "PC2")
```
From the above plot, we can say that the less violent crimes (blue points) in the top left, and violent crimes (red points) in the bottom right.

We also can see that the first principal component is more related to violent crimes than the second principal component.

### (3)

Train MSE is 0.2752071 and Test MES is 0.4248011.
We observe how the MSE of the test data is bigger than
the MSE obtained from train data. This makes sense, and the quality of the mode is not bad.

```{r 3.3,echo=FALSE}
######################  Assignment 3.3 #########################################
# Train and test data
n <- nrow(data)
set.seed(12345)
id <- sample(1:n, floor(n*0.5))
train_set <- data[id,]
test_set <- data[-id,]

# Scaling
scaler <- preProcess(train_set)
train <- predict(scaler,train_set)
test <- predict(scaler,test_set)

# Linear regression model and test data predictions
linmod <- lm(train$ViolentCrimesPerPop ~ ., train)
train_pred <- predict(linmod, train)
test_pred <- predict(linmod, test)

# Training and test data MSE

train_MSE <- mean((train$ViolentCrimesPerPop - train_pred)^2)
test_MSE <-  mean((test$ViolentCrimesPerPop - test_pred)^2)

cat("Train mean squared error:", train_MSE)
cat("Test mean squared error:", test_MSE)
```

### (4)

```{r 3.4.1,echo=FALSE,cache=TRUE}
######################  Assignment 3.4 #########################################
train_err <- list()
test_err <- list()
i <- 0

cost_fun <- function(theta_vec){
  theta <- matrix(theta_vec[1:100], nrow = 100, ncol = 1)
  
  X_train <- model.matrix(ViolentCrimesPerPop ~ .-1, train)
  X_test <- model.matrix(ViolentCrimesPerPop ~ .-1, test)

  train_mse <- sum(((X_train %*% theta) - train[,ncol(train)])^2) / nrow(train) 
  test_mse <- sum(((X_test %*% theta) - test[,ncol(train)])^2) / nrow(test)
  
  train_err[i] <<- train_mse
  test_err[i] <<- test_mse
  i <<- i  + 1 
  
  return(train_mse)
}

theta0 <- c(rep(0,100))

opt <- optim(theta0, fn = cost_fun, method="BFGS")
```


```{r 3.4.2,echo=FALSE}
# make data frame

mse_df <- data.frame(x = 1:length(train_err),
                     train_err = train_err, 
                     test_err = test_err)

x <- 1500:length(test_err)
y1 <- as.numeric(train_err[1500:length(train_err)])
y2 <- as.numeric(test_err[1500:length(test_err)])

plot(y1, type="l", col="red",ylim=c(0,1),xlab="Iteration",ylab="MSE",main = "MSE vs Iteration")
lines(y2, type="l", col="blue")
abline(v=which.min(test_err), col="green")
legend("topright", legend=c("Train MSE", "Test MSE"), col=c("red", "blue"), lty=1:2, cex=0.8)
```


```{r 3.4.3,echo=FALSE}

cat("Iteration for early stopping:", which.min(test_err))
cat("Train mean squared error:", train_err[[which.min(test_err)]])
cat("Test mean squared error:", test_err[[which.min(test_err)]])

```
According to the output of the code, an early stop can be done in `r which.min(test_err)` steps when the test error becomes minimal.

Comparing the train and test MSE in 3(3), we find that the test MSE error gets smaller, from `r test_MSE` to  `r test_err[[which.min(test_err)]]`.
However, train MSE gets a little bigger, from `r train_MSE` to `r train_err[[which.min(test_err)]]`.


\newpage
# Appendix: All code for this report

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```
