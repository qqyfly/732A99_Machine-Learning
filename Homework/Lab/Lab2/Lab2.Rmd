---
title: "Machine Learning Computer Lab 2 (Group A7)"
author: 
  - Qinyuan Qi(qinqi464)
  - Satya Sai Naga Jaya Koushik	Pilla (satpi345)
  - Daniele	Bozzoli(danbo826)  
date: "2023-11-26"
output: pdf_document
---

```{r setup1, echo=FALSE}
###########################  Init code For  Assignment 1 #######################
rm(list = ls())
library(plyr)
library(readr)
library(dplyr)
library(caret)
library(ggplot2)
library(repr)
library(glmnet)
library(rpart)
library(rpart.plot)

# read data
data <- read.csv("tecator.csv")
 
row_num <- nrow(data)
cols_num <- ncol(data)

# set data split ratio to 0.5, 0.5
ratio <- c(train = .5, test = .5)

# set random seed
set.seed(12345)

# split data to training and test dataset
train_id <- sample(1:row_num, floor(row_num * ratio[1]))
train_set <- data[train_id, ]

test_id <- setdiff(1:row_num, train_id)
test_set <- data[test_id, ]
```

## Assignment 1: Explicit regularization

### Answer:
### (1)

According to the output, the model generated use 100 channel features, and almost all the channels provide contributions to the target, however,  p value shows that only very limited channels are useful. And the  $MSE_{test} =  722.4294$ , $MSE_{training} = 0.005709117$. It means training data fit pretty well,however the test data fit not as expected, and the model overfit the data.

```{r 1.1}
######################  Assignment 1.1 #########################################

selected_x_columns <- grep(paste0("^", "Channel"), names(train_set), value = TRUE)

X_data_train <- train_set[, selected_x_columns]
Y_data_train <- train_set[names(train_set) == "Fat"]
train_data_set <- cbind(X_data_train, Y_data_train)

# fit linear model
lm_model <- lm(Fat ~ ., data = train_data_set)


# predict the test set and train set
X_data_test <- test_set[, selected_x_columns]
Y_data_test <- test_set[names(test_set) == "Fat"]
test_data_set <- cbind(X_data_test, Y_data_test)
predicted_fat_test <- predict(lm_model, test_data_set)
predicted_fat_train <- predict(lm_model, train_data_set)

# calc the mean value
test_mse <- mean((predicted_fat_test - test_data_set$Fat)^2) 
train_mse <- mean((predicted_fat_train - train_data_set$Fat)^2) 
cat("test_mse is:",test_mse,"\n")
cat("train_mse is:",train_mse,"\n")

summary(lm_model)
```

### (2)
The cost function in lasso regression is:

$$
MSE(y,y_pred) + \alpha * \sum_{i=1}^{n} |\theta_i|
$$

### (3)

According to the plot of lasso_reg, we choose top edge number to 3 which contains 3 curves and choose the appropriate lambda value , which is around -3.

```{r 1.3}
######################  Assignment 1.3 #########################################
# regularize the data before we fit the model
pre_proc_val <- preProcess(train_data_set, method = c("center", "scale"))
train_data_set <- predict(pre_proc_val, train_data_set)
test_data_set <- predict(pre_proc_val, test_data_set)

new_cols_num <- ncol(train_data_set)

X_data_train <- train_data_set[, 1:(new_cols_num - 1)]
Y_data_train <- train_data_set[, new_cols_num]
X_data_train <- as.matrix(X_data_train)

dummies <- dummyVars(Fat ~ ., data = train_data_set)

train_dummies <- predict(dummies, newdata = train_data_set)
test_dummies <- predict(dummies, newdata = test_data_set)

x <- as.matrix(train_dummies)
y_train <- train_data_set$Fat

x_test <- as.matrix(test_dummies)
y_test <- test_data_set$Fat

# set range of lambda, we can know the range of lambda, then set it manually
grid <- 10^seq(2, -3, by = -.1)

# when alpha = 1, it is lasso regression
lasso_reg <- glmnet(x, y_train, alpha = 1, family = "gaussian", lambda = grid)

summary(lasso_reg)

plot(lasso_reg, xvar = "lambda", label = TRUE)
```

### (4)

the cost function in ridge regression is:
$$
  MSE(y,y_pred) + \alpha * \sum_{i=1}^{n} (\theta_i)^2
$$

Compare 2 plots, we can find that for lasso regression, most of the features' coefficient convergence speed is very different. however, when using ridge regression, most of the features' coefficient convergence speed is almost same when log(lambda) increase.
```{r 1.4}
######################  Assignment 1.4 #########################################
# when alpha = 0, it is ridge regression
ridge_reg <- glmnet(x, y_train, alpha = 0, lambda = grid)

summary(ridge_reg)

plot(ridge_reg, xvar = "lambda", label = TRUE)
```

### (5)
The plot as follows, when log(lambda) increase, CV score increase.
Optimized lambda is -5.39019, and in this case, around 10 variables were chosen.

When lambda = -4, MSE is higher than when lambda=-5.39019. we can say that it results in a statistically significantly better prediction.

The scatter plot as follow.

Since almost all points are around the 45-degree line,it indicate accurate predictions.

```{r 1.5,warning=FALSE}
# when alpha = 1, it is lasso regression
cv_lasso_model <- cv.glmnet(x, y_train, alpha = 1) 
summary(cv_lasso_model)

# Extract lambda values and CV scores
lambda_values <- log(cv_lasso_model$lambda)
cv_scores <- cv_lasso_model$cvm

# Plot the dependence of CV score on log(lambda)
plot(cv_lasso_model)
plot(lambda_values, cv_scores, type = "b", xlab = "log(lambda)", ylab = "CV Score",main = "CV Score vs log(lambda)")

min_cv_lambda <- log(cv_lasso_model$lambda.min)
min_cv_score <- min(cv_scores)
cat("min_cv_lambda is:",min_cv_lambda,"\n")
points(min_cv_lambda, min_cv_score, col = "red", pch = 16, cex = 1.5)
text(min_cv_lambda, min_cv_score, "Minimum CV Score", pos = 3, col = "red")

predictions <- predict(cv_lasso_model, newx = as.matrix(x_test), s = "lambda.min")

plot(y_test, predictions, pch = 16, col = "red",
     xlab = "Original Values", ylab = "Predicted  Values",
     main = "Scatter Plot of Test Values for LASSO Model")
abline(a = 0, b = 1, col = "blue", lty = 2)
```

## Assignment 2: Decision trees and logistic regression for bank marketing

### Answer:

```{r setup2, include=FALSE}
###########################  Init code For  Assignment 1 #######################
rm(list = ls())

```

### (1)
```{r 2.1}
######################  Assignment 2.1 #########################################
# read data
data <- read.csv("bank-full.csv",header = TRUE, sep = ";")

row_num <- nrow(data)
cols_num <- ncol(data)

# set data split ratio to 0.4, 0.3, 0.3
ratio <- c(train = .4, validate = 0.3, test = .3)

# set random seed
set.seed(12345)

# split data to training,validate and test dataset
train_id <- sample(1:row_num, floor(row_num * ratio[1]))
train_set <- data[train_id, ]

# set random seed
set.seed(12345)

validation_test_id <- setdiff(1:row_num, train_id)
validation_id <- sample(validation_test_id, floor(row_num * ratio[2]))
validation_set <- data[validation_id, ]

test_id <- setdiff(validation_test_id, validation_id)
test_set <- data[test_id, ]

```

### (2)

Decision trees are as follows.

misclassification is listed below.

||misclassification rate of train|misclassification rate of validation|
|:-:|:-:|:-:|
|a|0.0995355|0.0995355|
|b|0.1142446|0.1207697|
|c|0.06287326|0.1041805|

According to the data in the table above, the first one has a good misclassification rate of train and validation.
the last one has a good misclassification rate of train but a  little bit large  misclassification rate on validation set. b will not be considered, since both of values are biggest among 3 methods.

```{r 2.2}
######################  Assignment 2.2 #########################################
tree_a <- rpart(y ~ ., data = train_set, method = "class")
tree_b <- rpart(y ~ ., data = train_set, method = "class", control = rpart.control(minbucket = 7000))
tree_c <- rpart(y ~ ., data = train_set, method = "class", control = rpart.control(cp = 0.0005))

train_predictions_a <- predict(tree_a, train_set, type = "class")
validation_predictions_a <- predict(tree_a, validation_set, type = "class")

train_predictions_b <- predict(tree_b, train_set, type = "class")
validation_predictions_b <- predict(tree_b, validation_set, type = "class")

train_predictions_c <- predict(tree_c, train_set, type = "class")
validation_predictions_c <- predict(tree_c, validation_set, type = "class")

# misclassification rates
train_misclassification_rate_a <- mean(train_predictions_a != train_set$y)
validation_misclassification_rate_a <- mean(validation_predictions_a != validation_set$y)

train_misclassification_rate_b <- mean(train_predictions_b != train_set$y)
validation_misclassification_rate_b <- mean(validation_predictions_b != validation_set$y)

train_misclassification_rate_c <- mean(train_predictions_c != train_set$y)
validation_misclassification_rate_c <- mean(validation_predictions_c != validation_set$y)



cat("train_misclassification_rate_a is ",train_misclassification_rate_a,"\n")
cat("validation_misclassification_rate_a is ",validation_misclassification_rate_a,"\n")

cat("train_misclassification_rate_b is ",train_misclassification_rate_b,"\n")
cat("validation_misclassification_rate_b is ",validation_misclassification_rate_b,"\n")

cat("train_misclassification_rate_c is ",train_misclassification_rate_c,"\n")
cat("validation_misclassification_rate_c is ",validation_misclassification_rate_c,"\n")

rpart.plot(tree_a)
rpart.plot(tree_b)
rpart.plot(tree_c)
```

### (3)
```{r 2.3}
######################  Assignment 2.3 #########################################
```

### (4)
```{r 2.4}
######################  Assignment 2.4 #########################################
```

### (5)
```{r 2.5}
######################  Assignment 2.5 #########################################
```

### (6)
```{r 2.6}
######################  Assignment 2.6 #########################################

```

## Assignment 3. Principal components and implicit regularization

### Answer:

```{r setup3, include=FALSE}
###########################  Init code For  Assignment 3 #######################
rm(list=ls(all.names = T))
library(ggplot2)
library(caret)
set.seed(12345)
data <- read.csv("communities.csv")
```

### (1)
According to the output, we need 35 components to obtain at least 95% of variance in the data. The proportion of variation explained by first and second principal components are 0.2501699 and 0.1693597 respectively.

```{r 3.1}
######################  Assignment 3.1 #########################################
n <- nrow(data)
features <- data[, -101]

s_features <- scale(features)

S <- (t(s_features) %*% s_features)/n  # sample covariance matrix

Eig <- eigen(S) 

# eigen in descending order
s_indx <- order(Eig$values, decreasing = TRUE)
s_eig <- Eig$values[s_indx]

# cumulative explained variance
cum_var <- cumsum(s_eig) / sum(s_eig)

q_95 <- which(cum_var >= 0.95)[1]  # q for 95% var

first_two_components <- Eig$vectors[, 1:2]  # first two PC

# proportion of variation explained by each of the first two components
PC1_var <- s_eig[1] / sum(s_eig)
PC2_var <- s_eig[2] / sum(s_eig)

cat("Number of components needed for 95% variance:", q_95, "
Proportion of variation explained by the first component:", PC1_var, "
Proportion of variation explained by the second component:", PC2_var, "\n")
```

### (2)

We observe from the plot how the variable "state", the first feature of PC1, 
explains most of the data. The other variables carry significantly less 
explanation. The other 4 features that contribute the most are features 93 
(PctBornSameState), 61 (PctSpeakEnglOnly), 5 (racePctWhite) and 76 
(PersPerRentOccHous).

```{r 3.2}
######################  Assignment 3.2 #########################################
PCA <- princomp(features)
L <- PCA$loadings
plot(L[,1], xlim=c(0,20))
highest5 <- order(L[,1], decreasing=TRUE)[1:5]
plot(L[,1], L[,2], main="PCA Scores")
```

### (3)

Train MSE is 0.2752071 and Test MES is 0.4248011.
We observe how the MSE of the test data is significantly bigger than
the MSE obtained from train data. This might happen because the model chosen is overfitting our given data.

```{r 3.3}
######################  Assignment 3.3 #########################################
# train and test data
id <- sample(1:n, floor(n*0.5))
trn <- data[id,]
tst <- data[-id,]

# scaling
scaler <- preProcess(trn)
trainS <- predict(scaler,trn)
testS <- predict(scaler,tst)

# linear regression model and test data predictions
linmod <- lm(trainS$ViolentCrimesPerPop ~ ., trainS)
test_pred <- predict(linmod, testS[,-101])

# training and test data MSE
train_MSE <- mean((trainS$ViolentCrimesPerPop - linmod$fitted.values)^2)
test_MSE <- mean((testS$ViolentCrimesPerPop - test_pred)^2)

cat("Train mean squared error:", train_MSE, "\nTest mean squared error:", test_MSE)

```

### (4)

Looking at the plots we can see how the errors converge to zero
after 1700 iterations (approx 1200 in the second graph but considering that we took off
the first 500). Hence 1700 is the optimal iteration number to get good results.
The following iterations do not significally improve our model
and hence can led us to overfitting.


```{r 3.4}
######################  Assignment 3.4 #########################################
train_new <- as.matrix(trainS[,-101])  # training data - response variable
train_r <- trainS[,101] # training response variable

test_new <- as.matrix(testS[,-101])  # test data - response variable
test_r <- trainS[,101] # test response variable

# error vectors for training and test data
train_e <- c()
test_e <- c()

costfun <- function(theta_vec){
  train_cost <- mean(((train_new %*% theta_vec) - train_r)^2)
  train_e <<- c(train_e, train_cost)
  test_cost <- mean(((test_new %*% theta_vec) - test_r)^2)
  test_e <<- c(test_e, test_cost)
  return(train_cost)
}

theta0 <- rep(0,100)
opt <- optim(theta0, method="BFGS", costfun)

opt$val

par(mfrow=c(1,2))

plot(ylab="errors", xlab="iteration",train_e, type = "l", main="Errors")
lines(test_e, col="red")
legend("topright", legend = c("Train errors", "Test errors"), 
col = c("black", "red"), lty = 1, cex = 0.5)

plot(ylab="errors", xlab="iteration",train_e[-c(1:500)][1:2000], 
type = "l", main="Zoom on errors")
lines(test_e[-c(1:500)][1:2000], col="red")
legend("topright", legend = c("Train errors", "Test errors"), 
col = c("black", "red"), lty = 1, cex = 0.5)

```



\newpage
# Appendix: All code for this report

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```
