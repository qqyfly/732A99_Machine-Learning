---
title: "Machine Learning Computer Lab 1 (Group A7)"
author: 
  - Qinyuan Qi(qinqi464)
  - Satya Sai Naga Jaya Koushik	Pilla (satpi345)
  - Daniele	Bozzoli(danbo826)  
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup1, include=FALSE}
###########################  Init code For Assignment 1 ########################
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
library(kknn)
```

## Assignment 1: Handwritten digit recognition with K- nearest neighbors  (Solved by Qinyuan Qi - qinqi464)

### Answer:

### (1)

The following code will input the data and divide the data into training, validation and test sets.

```{r 1.1}
######################  Assignment 1.1 #########################################
# read data
data <- read.csv("optdigits.csv")
row_num <- nrow(data)
cols_num <- ncol(data)

# set the last column name as "label_value" since we need to create a formula later
names(data)[cols_num] <- "label_value"

# set data split ratio to 0.5, 0.25 and 0.25
ratio <- c(train = .5, test = .25, validate = .25)

# data pre-processing
# columns 1-64 are number based and do not need to be normalized, last column
# is integer represent number from 0-9 which don't need to process again

# set random seed
set.seed(12345)

# split data to training, test and validation set
train_id <- sample(1:row_num, floor(row_num * ratio[1]))
train_set <- data[train_id, ]

set.seed(12345)
test_val_id <- setdiff(1:row_num, train_id)
valid_id <- sample(test_val_id, floor(row_num * ratio[2]))
valid_set <- data[valid_id, ]

test_id <- setdiff(test_val_id, valid_id)
test_set <- data[test_id, ]
```

### (2)

The code(Check appendix) contain a function call to kknn with k=30 and kernel = ”rectangular”. We calculate the confusion matrices and the misclassification errors.

```{r 1.2, include=FALSE}
######################  Assignment 1.2 #########################################
# calculate error rate for k = 30
k_value <- 30

train_kknn <- kknn::kknn(label_value ~ .,
                       train_set,
                       valid_set,
                       k = k_value,
                       kernel = "rectangular")

predict_data <- predict(train_kknn,newdata=valid_set)

# generate confusion matrix
confusion_matrices <- table(round(predict_data), valid_set$label_value)

# calculate accuracy
accuracy <- sum(diag(confusion_matrices)) / sum(confusion_matrices)

# calculate error rate
error_rate <- 1 - accuracy
```

The confusion_matrices is as follows:

|   | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| 0 |98 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| 1 | 0 |70 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 |
| 2 | 1 |26 |87 | 0 | 1 | 0 | 2 | 0 | 1 | 0 |
| 3 | 0 | 4 |11 |52 | 3 | 0 | 0 | 0 | 1 | 2 |
| 4 | 0 | 1 | 8 |25 |74 | 3 | 0 | 1 | 2 | 0 |
| 5 | 0 | 1 | 4 |12 |18 |66 | 1 | 0 | 4 | 2 |
| 6 | 0 | 0 | 1 | 2 | 6 |19 |80 | 5 |10 | 6 |
| 7 | 0 | 2 | 1 | 0 | 5 | 4 | 0 |90 |18 |13 |
| 8 | 0 | 1 | 0 | 0 | 1 | 1 | 0 | 1 |48 |42 |
| 9 | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 0 |20 |

As what the data show in the confusion matrix, we can find that the accuracy of number 0,1,2,9 are super high. For most of the numbers, the accuracy are acceptable.And we got a total accuracy rate of 0.7172775, and total error rate is 0.2827225.

### (3)

We use the following code to generate heatmaps. By show 205 heat images one by 
one, we found first 2 images(index 18,50) looks like 8, but next 3 images(index 1, 2,55) not like 8 at all.

The image is as follows.The code attached as appendix.

```{r 1.3,echo = FALSE}
######################  Assignment 1.3 #########################################
# Find all the 8 images
eight_images <- train_set[train_set$label_value == 8, ]

# get the number of 8 images in training data, we got 205 images
eight_images_number <- nrow(eight_images)

image_number  <- 18
eight_images_matrix <- matrix(c(as.numeric(eight_images[image_number, 1:64])), ncol = 8)
heatmap(x = eight_images_matrix, Rowv = NA, Colv = NA)

image_number  <- 50
eight_images_matrix <- matrix(c(as.numeric(eight_images[image_number, 1:64])), ncol = 8)
heatmap(x = eight_images_matrix, Rowv = NA, Colv = NA)

image_number  <- 1
eight_images_matrix <- matrix(c(as.numeric(eight_images[image_number, 1:64])), ncol = 8)
heatmap(x = eight_images_matrix, Rowv = NA, Colv = NA)

image_number  <- 2
eight_images_matrix <- matrix(c(as.numeric(eight_images[image_number, 1:64])), ncol = 8)
heatmap(x = eight_images_matrix, Rowv = NA, Colv = NA)

image_number  <- 55
eight_images_matrix <- matrix(c(as.numeric(eight_images[image_number, 1:64])), ncol = 8)
heatmap(x = eight_images_matrix, Rowv = NA, Colv = NA)

```

### (4)

We fit the KNN with k = 1 to 30, we plot the error rates as follows, the code is attached in appendix.

When K increase, the model not become very complex,it seems it does not change a lot when k >= 2
And according to the graph, we can know that the optimal K is 1.

```{r 1.4,echo = FALSE}
######################  Assignment 1.4 #########################################
# calculate error rate for different k values
error_rates_valid <- rep(0, 30)

for(k_value in 1:30){
  # apply kknn function 
  valid_kknn <- kknn::kknn(label_value ~ .,
                       train_set,
                       valid_set,
                       k = k_value,
                       kernel = "rectangular")

 predict_data_valid <- predict(valid_kknn,newdata=valid_set)
 
  # generate confusion matrix
 confusion_matrices_valid <- table(round(predict_data_valid), valid_set$label_value) 

 # calculate accuracy
 accuracy_valid <- sum(diag(confusion_matrices_valid)) / sum(confusion_matrices_valid)

  # calculate error rate
  error_rates_valid[k_value] <- 1 - accuracy_valid
 
}

# plot the error rate graph
k <- 1:30

error_rate_data <- data.frame(k, error_rates_valid)
ggplot2::ggplot() +
  ggplot2::geom_line(error_rate_data,mapping = ggplot2::aes(x=k,y=error_rates_valid)) +  
  ggplot2::labs(x = "K",y="error rate")
```

### (5)

The cross-entropy is defined as $-\sum_{i=1}^{N}\sum_{m=1}^{M}I(Y_{i}=C_{m}) * log\hat{p}(y_{i}=C_{m})$.

So we will calculate it using the following code(Not implemented).

```{r 1.5,echo = FALSE}
#error_rates_cross <- rep(30, 0)

#for(k_value in 1:30){
#  # apply kknn function 
#  train_kknn <- train.kknn(label_value ~ .,
#                       data = train_set,
#                       kmax = k_value,
#                       kernel = "rectangular")

#  predict_data_test <- predict(train_kknn,newdata=test_set)
  
#  confusion_matrices_test <- table(round(predict_data_test), test_set$label_value)
  
  # calculate cross-entropy
  #ce <- 0
  #for(i in 1:10){
  #  total_count <- sum(confusion_matrices_test[i])
  #  for(j in 1:10){
  #    p <- confusion_matrices_test[i,j] / total_count
  #    print(p)
  #    ce <- ce + log(p) * confusion_matrices_test[i,j]
  #    
  #  }
  #}
  #error_rates_cross[k_value] <- ce * (-1)
#}

#k <- 1:30

#error_rate_data <- data.frame(k,error_rates_cross)
#ggplot2::ggplot() + 
#  ggplot2::geom_line(error_rate_data,mapping=ggplot2::aes(x=k,y=error_rates_cross),colour="blue") + 
#  ggplot2::labs(x = "K",y="error rate")

```


The reason why we use cross-entropy is more suitable is:
1) It is sensitive to the predicted probabilities.
2) In multinomial distribution, the predicted value will more likely to be explained as a probability, and cross-entropy loss function is designed to compare the probability based prediction.


## Assignment 2: Linear regression and ridge regressions (Solved by Satya Sai Naga Jaya Koushik Pilla)

### Answer:


```{r setup2, include=FALSE}
###########################  Init code For Assignment 2 ########################
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)

###########################  Common Functions ##################################
# normalize data
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}
```

### (1)

We read the data and divide data into training and test data (60/40) and normalize them.
```{r 2.1}
###########################  Assignment 2.1 ####################################
# Load the data
data <- read.csv("parkinsons.csv")
row_num <- nrow(data)
cols_num <- ncol(data)

# Divide the data into training and test data (60/40)
# set data split ratio
ratio <- c(train = .6, test = .4)

# set random seed
set.seed(12345)

# split data
train_id <- sample(1:row_num, floor(row_num * ratio[1]))
train_set <- data[train_id, ]
test_id <- setdiff(1:row_num, train_id)
test_set <- data[test_id, ]

# normalize data.
train_set <- as.data.frame(lapply(train_set, normalize))
test_set <- as.data.frame(lapply(test_set, normalize))
```

### (2)

We create a linear model using following code, we get the parameters as show below.

```{r 2.2}
###########################  Assignment 2.2 ####################################
# Linear regression model

# apply linear regression
model <- lm(motor_UPDRS ~ ., data = train_set)

# predict the test value using the linear regression just created
test_pred <- predict(model, test_set)

# calculate test MSE
test_mse <- mean((test_pred - test_set$motor_UPDRS)^2)
model
```

And we get test MSE = 0.005370424
we know that Shimmer.APQ3 and Shimmer.DDA contribute significantly to the model

### (3)

Functions implemented as below.
```{r 2.3}
###########################  Assignment 2.3 ####################################
# Loglikelihood function
loglikelihood <- function(theta, sigma) {
  n <- length(Y)
  log_likelihood_values <- -0.5 * (log(2 * pi * sigma^2) + ((Y - theta %*% X) / sigma)^2)
  return(total_log_likelihood <- sum(log_likelihood_values))
}

# ridge
ridge <- function(param) {
  param_length <- length(param)
  theta <- param[1:param_length-2]
  sigma <- param[param_length-1]
  lambda <- param[param_length] 
  loglikelihood_result <- loglikelihood(theta, sigma)
  ridge_penalty <- lambda * sum(theta^2)
  return(loglikelihood_result - ridge_penalty)
}

# ridgeopt
ridgeopt <- function(theta, sigma, lambda) {
  size_theta <- length(theta)
  param <- rep(0,size_theta+2)
  for(i in 1:size_theta){
    param[i] <- theta[i]
  }
  param[size_theta + 1] <- sigma
  param[size_theta + 2] <- lambda
  ridge_result <- optim(par = param, fn = ridge, method="BFGS")
  
}

# df
df <- function(X,lambda) {
  n <- nrow(X)
  # calculate hat_matrix
  hat_matrix <- X %*% solve(t(X) %*% X + lambda * diag(ncol(X)))
  # get degree of the hat_matrix
  df_ridge <- sum(diag(hat_matrix))
  return(df_ridge)
}
```

### (4)

Functions implemented as below.
```{r 2.4}
###########################  Assignment 2.4 ####################################

#sigma <- 0.01
#theta <- rep(1,21)
#X <- train_set[] 
#lambda <- 1
#ridgeopt(theta,sigma,lambda)

#lambda <- 100
#ridgeopt(theta,sigma,lambda)

#lambda <- 1000
#ridgeopt(theta,sigma,lambda)

```

## Assignment 3. Logistic regression and basis function expansion (Solved by Daniele	Bozzoli)

### Answer:

```{r setup3, include=FALSE}
###########################  Init code For Assignment 3 ########################
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
```

### (1)

```{r 3.1}
data <- read.csv("pima-indians-diabetes.csv")

age <- data[, 8]
plasma <- data[, 2]

names(data)[9] <- "diabetes"
names(data)[2] <- "plasma"
names(data)[8] <- "age"

x <- age
y <- plasma
df <- data.frame(x, y)

ggplot2::ggplot(df, ggplot2::aes(age, plasma)) +
  ggplot2::geom_point(ggplot2::aes(colour = data[, 9]))+
  ggplot2::labs(x = "Age", y="Plasma glucose")

```

We observe how a lot of people without diabete is highly concentrated in the lower 
part of the age axis, explaining how younger individuals tend to be non-diabetic,
on the other hand, we notice how especially people with higher plasma glucose 
concentration tend to be diabetic, less concentrated in a specific age group, more
spread across the x-age axis.

### (2)(3)

Code as below.
Looking at the graph, we notice how the classification is not very accurate. Though, it 
catches the fact that people with higher plasma glucose concentration are more 
commonly diabetic. We obtain a misclassification error = 0.266.

```{r 3.2_3.3}

model <- glm(diabetes ~ plasma + age, fam = binomial(link="logit"), data = data)
summary(model)

# r = 0.5

c1 <- fitted(model) >= .5

missC_error <- sum(data$diabetes != c1) / nrow(data)
cat("Misclassification error:", missC_error, "\n")

ggplot2::ggplot(df, ggplot2::aes(age, plasma)) +
  ggplot2::geom_point(ggplot2::aes(colour = c1))+
  ggplot2::labs(x = "Age", y="Plasma glucose")     # T = Diabetic, F = Non-diabetic

```


### (4)

It looks like the line that splits the predicted observations into diabetic
and non diabetic into the two cases ( r= 0.2 and r=0.8) has the same slope, but
different intercept. Both cases are pretty bad predictors as 0.2 and 0.8 are
respectively too low and too high to get a good classification for our case.

```{r 3.4}

# r = 0.2

c2 <- fitted(model) >= .2

ggplot2::ggplot(df, ggplot2::aes(age, plasma)) +
  ggplot2::geom_point(ggplot2::aes(colour = c2))+
  ggplot2::labs(x = "Age", y="Plasma glucose")

# r = 0.8

c3 <- fitted(model) >= .8

ggplot2::ggplot(df, ggplot2::aes(age, plasma)) +
  ggplot2::geom_point(ggplot2::aes(colour = c3))+
  ggplot2::labs(x = "Age", y="Plasma glucose")

```

### (5)

The outcome of this analysis is better than the first case, as we obtain a 
lower misclassification error than before, and also graphically we notice
how this version captures what we were saying in the first part of the 
analysis, how people with higher plasma glucose concentration are more 
exposed to the risk of having diabete. We also notice how we strangely get a 
misclassificated predicted value in the bottom right corner of the graph.

```{r 3.5}

# Adding new variables, logit function with r= 0.5

z1 <- data$plasma^4
z2 <- data$plasma^3 * data$age
z3 <- data$plasma^2 * data$age^2
z4 <- data$plasma^1 * data$age^3
z5 <- data$age^4

newdata <- cbind(data, z1, z2, z3, z4, z5)

newmodel <- glm(diabetes ~ plasma + age + z1 + z2 + z3 + z4 + z5 , family = binomial(link = "logit") , data = newdata)

summary(newmodel)

c4 <- fitted(newmodel) >= .5

new_missC_error <- sum(data$diabetes != c4) / nrow(data)
cat("Misclassification error:", new_missC_error, "\n")

ggplot2::ggplot(df, ggplot2::aes(age, plasma)) +
  ggplot2::geom_point(ggplot2::aes(colour = c4))+
  ggplot2::labs(x = "Age", y="Plasma glucose")

```




\newpage
# Appendix: All code for this report

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```
